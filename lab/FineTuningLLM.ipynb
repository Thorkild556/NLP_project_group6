{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbe91b9-6e04-4223-8ddb-5376063aa2c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:49:05.104747Z",
     "iopub.status.busy": "2025-11-03T23:49:05.103400Z",
     "iopub.status.idle": "2025-11-03T23:49:05.110838Z",
     "shell.execute_reply": "2025-11-03T23:49:05.109514Z",
     "shell.execute_reply.started": "2025-11-03T23:49:05.103400Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import parse_qs\n",
    "from transformers import T5Tokenizer\n",
    "from evaluate import load\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import scipy\n",
    "import math\n",
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337388d4-56be-4596-b40b-e267fccf10c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:18.453761Z",
     "iopub.status.busy": "2025-11-03T23:47:18.452725Z",
     "iopub.status.idle": "2025-11-03T23:47:22.467532Z",
     "shell.execute_reply": "2025-11-03T23:47:22.466527Z",
     "shell.execute_reply.started": "2025-11-03T23:47:18.452725Z"
    }
   },
   "outputs": [],
   "source": [
    "data_set = load_dataset(\"trl-lib/tldr\")\n",
    "ds = data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0585c13f-3313-4a48-bfb0-ffc776549b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:22.468798Z",
     "iopub.status.busy": "2025-11-03T23:47:22.468798Z",
     "iopub.status.idle": "2025-11-03T23:47:57.510524Z",
     "shell.execute_reply": "2025-11-03T23:47:57.510524Z",
     "shell.execute_reply.started": "2025-11-03T23:47:22.468798Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = {}\n",
    "for set_type in data_set.keys():\n",
    "  ds[set_type] = pd.DataFrame(data_set[set_type])\n",
    "  ds[set_type][\"prompt_post\"] = ds[set_type].prompt.str.extract(\n",
    "      r'POST: ((.|\\n)*)\\nTL;DR:', expand=False\n",
    "  ).iloc[:, 0]\n",
    "  ds[set_type][\"prompt_title\"] = ds[set_type].prompt.str.extract(\n",
    "    r'TITLE: ((.|\\n)*)\\n\\nPOST:', expand=False\n",
    "  ).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d101a1b5-b669-4d55-8d95-3d75afb36aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:57.511554Z",
     "iopub.status.busy": "2025-11-03T23:47:57.511554Z",
     "iopub.status.idle": "2025-11-03T23:47:57.516203Z",
     "shell.execute_reply": "2025-11-03T23:47:57.515198Z",
     "shell.execute_reply.started": "2025-11-03T23:47:57.511554Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e66bee-684d-4cd0-a3e4-d8b686778d39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:57.517388Z",
     "iopub.status.busy": "2025-11-03T23:47:57.516203Z",
     "iopub.status.idle": "2025-11-03T23:47:57.940618Z",
     "shell.execute_reply": "2025-11-03T23:47:57.939615Z",
     "shell.execute_reply.started": "2025-11-03T23:47:57.517388Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiha\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49f73a6-697b-4dea-8bdd-60f5625416bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:57.942815Z",
     "iopub.status.busy": "2025-11-03T23:47:57.941694Z",
     "iopub.status.idle": "2025-11-03T23:47:57.966096Z",
     "shell.execute_reply": "2025-11-03T23:47:57.965590Z",
     "shell.execute_reply.started": "2025-11-03T23:47:57.942815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'completion', 'prompt_post', 'prompt_title'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples = ds[\"train\"].sample(1000)\n",
    "validation_samples = ds[\"validation\"].sample(100)\n",
    "training_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61df78ab-96ca-4f26-a391-891de9596639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:57.967616Z",
     "iopub.status.busy": "2025-11-03T23:47:57.967616Z",
     "iopub.status.idle": "2025-11-03T23:47:57.972426Z",
     "shell.execute_reply": "2025-11-03T23:47:57.972426Z",
     "shell.execute_reply.started": "2025-11-03T23:47:57.967616Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples.loc[:, \"prompt\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # we had max 450 words, so we must be fine. wiht 512 with limit\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples.loc[:, \"completion\"].values.tolist(), max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59617fee-ca8a-4eaf-bda6-ad7a0297f4ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:47:57.975277Z",
     "iopub.status.busy": "2025-11-03T23:47:57.973863Z",
     "iopub.status.idle": "2025-11-03T23:48:02.966477Z",
     "shell.execute_reply": "2025-11-03T23:48:02.965476Z",
     "shell.execute_reply.started": "2025-11-03T23:47:57.975277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saiha\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "p_training_samples = Dataset.from_dict(preprocess_function(training_samples))\n",
    "p_validation_samples = Dataset.from_dict(preprocess_function(validation_samples))\n",
    "rouge = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab4c0ff-526e-44ce-a660-f8df7824a33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:48:02.967552Z",
     "iopub.status.busy": "2025-11-03T23:48:02.967552Z",
     "iopub.status.idle": "2025-11-03T23:48:04.568435Z",
     "shell.execute_reply": "2025-11-03T23:48:04.567096Z",
     "shell.execute_reply.started": "2025-11-03T23:48:02.967552Z"
    }
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267b48bb-62b4-4b84-9970-bf16f8e4fb34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:49:43.771752Z",
     "iopub.status.busy": "2025-11-03T23:49:43.771752Z",
     "iopub.status.idle": "2025-11-03T23:49:43.778188Z",
     "shell.execute_reply": "2025-11-03T23:49:43.778188Z",
     "shell.execute_reply.started": "2025-11-03T23:49:43.771752Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # If model returns tuple (logits, etc.)\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Ensure lists of ints (remove nested levels)\n",
    "    preds = [p.tolist() if hasattr(p, \"tolist\") else p for p in preds]\n",
    "    labels = [l.tolist() if hasattr(l, \"tolist\") else l for l in labels]\n",
    "\n",
    "    # Some preds are 3D (e.g. [batch, seq, beam]) â€” fix that\n",
    "    if isinstance(preds[0][0], list):\n",
    "        preds = [p[0] for p in preds]  # take first beam\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 with pad_token_id before decoding labels\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa735e36-039b-42b9-855c-15f7abd010d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:49:44.221745Z",
     "iopub.status.busy": "2025-11-03T23:49:44.221745Z",
     "iopub.status.idle": "2025-11-03T23:49:44.418429Z",
     "shell.execute_reply": "2025-11-03T23:49:44.417429Z",
     "shell.execute_reply.started": "2025-11-03T23:49:44.221745Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mini-trained-model-for-project\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=p_training_samples,\n",
    "    eval_dataset=p_validation_samples,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153d42e7-6c6f-4b38-a5ba-1ed0eefb90b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:49:46.066585Z",
     "iopub.status.busy": "2025-11-03T23:49:46.065545Z",
     "iopub.status.idle": "2025-11-04T00:15:50.321198Z",
     "shell.execute_reply": "2025-11-04T00:15:50.319659Z",
     "shell.execute_reply.started": "2025-11-03T23:49:46.066585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/375 19:38 < 39:26, 0.11 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1859\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1857\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2298\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2295\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2297\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2298\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2302\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2662\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2660\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m2662\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2663\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   2665\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3467\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3464\u001b[39m start_time = time.time()\n\u001b[32m   3466\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   3471\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   3472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3475\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3477\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   3478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3719\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   3715\u001b[39m         metrics = \u001b[38;5;28mself\u001b[39m.compute_metrics(\n\u001b[32m   3716\u001b[39m             EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n\u001b[32m   3717\u001b[39m         )\n\u001b[32m   3718\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3719\u001b[39m         metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3720\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3721\u001b[39m     metrics = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(eval_pred)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     14\u001b[39m     preds = [p[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m preds]  \u001b[38;5;66;03m# take first beam\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m decoded_preds = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Replace -100 with pad_token_id before decoding labels\u001b[39;00m\n\u001b[32m     19\u001b[39m labels = [[(l \u001b[38;5;28;01mif\u001b[39;00m l != -\u001b[32m100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m tokenizer.pad_token_id) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3771\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3748\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3749\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3752\u001b[39m     **kwargs,\n\u001b[32m   3753\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3754\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3755\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3756\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3769\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3770\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m   3772\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3773\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3774\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3778\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\n\u001b[32m   3779\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3772\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3748\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3749\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3752\u001b[39m     **kwargs,\n\u001b[32m   3753\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3754\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3755\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3756\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3769\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3770\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3772\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3773\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3774\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3778\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3779\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3811\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3808\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3809\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3811\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3816\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils.py:1001\u001b[39m, in \u001b[36mPreTrainedTokenizer._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    991\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode\u001b[39m(\n\u001b[32m    992\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    993\u001b[39m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    997\u001b[39m     **kwargs,\n\u001b[32m    998\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_source_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     filtered_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m     legacy_added_tokens = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m._added_tokens_encoder.keys()) - \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.all_special_tokens) | {\n\u001b[32m   1003\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.additional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(token) >= \u001b[38;5;28mself\u001b[39m.vocab_size\n\u001b[32m   1004\u001b[39m     }\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[32m   1007\u001b[39m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils.py:982\u001b[39m, in \u001b[36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m    980\u001b[39m         tokens.append(\u001b[38;5;28mself\u001b[39m._added_tokens_decoder[index].content)\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m         tokens.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:408\u001b[39m, in \u001b[36mT5Tokenizer._convert_id_to_token\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m    407\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     token = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msp_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIdToPiece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\sentencepiece\\__init__.py:1183\u001b[39m, in \u001b[36m_batchnize.<locals>._batched_func\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1181\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m [_func(\u001b[38;5;28mself\u001b[39m, n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m arg]\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Hosted Experiments\\NLPProject\\.venv\\Lib\\site-packages\\sentencepiece\\__init__.py:1176\u001b[39m, in \u001b[36m_batchnize.<locals>._func\u001b[39m\u001b[34m(v, n)\u001b[39m\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_func\u001b[39m(v, n):\n\u001b[32m   1175\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (n < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n >= v.piece_size()):\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mpiece id is out of range.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1177\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m func(v, n)\n",
      "\u001b[31mIndexError\u001b[39m: piece id is out of range."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37b21cd2-51bf-4ab8-91ce-0b0227cc6bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:49:08.421884Z",
     "iopub.status.busy": "2025-11-03T23:49:08.421884Z",
     "iopub.status.idle": "2025-11-03T23:49:09.836275Z",
     "shell.execute_reply": "2025-11-03T23:49:09.834275Z",
     "shell.execute_reply.started": "2025-11-03T23:49:08.421884Z"
    }
   },
   "outputs": [],
   "source": [
    "non_sense = \"https://www.youtube.com/watch?v=YcSP1ZUf1eQ&list=RDYcSP1ZUf1eQ&start_radio=1\"\n",
    "def get_video_key(p_url):\n",
    "    parsed_url = urlparse(p_url)\n",
    "    captured_value = parse_qs(parsed_url.query)['v'][0]\n",
    "    return captured_value\n",
    "\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "fc = ytt_api.fetch(get_video_key(non_sense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8776a-75c1-4aac-94dc-10d0d79ba676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
