{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iuzUsqiCCEiy",
    "outputId": "cd4155e3-52ff-4332-ed8c-aa7bdf644d2d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers torch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3Gp8_uFAIa0u",
    "outputId": "312ad154-845a-4ce1-e2f6-aa583e8a9c0d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jsBLvi_EG_fr",
    "outputId": "72b6f71f-cd42-4863-d650-e5f456a6ec3c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Collecting torch==2.9.1 (from xformers)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->xformers)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.9/122.9 MB\u001B[0m \u001B[31m19.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m47.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.0/88.0 MB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m954.8/954.8 kB\u001B[0m \u001B[31m41.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.1/193.1 MB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m63.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.6/63.6 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.5/267.5 MB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m288.2/288.2 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.3/39.3 MB\u001B[0m \u001B[31m50.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.5/170.5 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.26.1-py3-none-any.whl (517 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m517.4/517.4 kB\u001B[0m \u001B[31m43.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.1/59.1 MB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, bitsandbytes, trl\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1 trl-0.26.1 xformers-0.0.33.post2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "id": "rdX_zO20JVq2",
    "outputId": "ccbca90d-9b9d-48e0-aab9-48893d722e2b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch"
   ],
   "metadata": {
    "id": "huQ3Z7DuC54h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"We would be using this device:\", device)"
   ],
   "metadata": {
    "id": "jhOjeEb4DCW9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6b0dbeb8-aeeb-46dd-e732-5e64432f2910"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the TL;DR Dataset\n",
    "\n",
    "we have saved the TL;DR Dataset in the JSONL format"
   ],
   "metadata": {
    "id": "SyTCKMBEGZWg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load JSONL data (TL;DR Dataset)\n",
    "print(\"\\n[1/8] Loading dataset...\")\n",
    "data = []\n",
    "with open(\"proc_tldr.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"messages\": [item[\"messages\"] for item in data]\n",
    "})\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset)} examples\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsrNdOKbGLzr",
    "outputId": "d47fb7a1-82f2-412d-b5ae-0dd69c6b60e1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[1/8] Loading dataset...\n",
      "✓ Loaded 6944 examples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing"
   ],
   "metadata": {
    "id": "MIHDfzqiMhCm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Split dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "print(f\"✓ Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ieh8zxBGf5S",
    "outputId": "f8326cc8-7742-4fa2-e60b-f539e3d0b408"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Train: 6249 | Val: 695\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[2/8] Configuring 8-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDxbMKb1Gs9O",
    "outputId": "4ce39fa2-941a-413a-86d4-5ef6c440de4f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[2/8] Configuring 8-bit quantization...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## Note\n",
    "\n",
    "we have login inside hugging face and make sure we request for access to use the LLaMA 3.2 3B model"
   ],
   "metadata": {
    "id": "SbHyajW7Ms33"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!huggingface-cli login"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS_KmcPPfkc3",
    "outputId": "565c3e29-bedd-4dc6-d086-5e69489b75c3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001B[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "print(\"\\n[3/8] Loading LLaMA 3.2 3B model...\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# LLaMA models usually do not have a pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded in 8-bit\")\n",
    "print(\"✓ Model size: ~3B parameters\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590,
     "referenced_widgets": [
      "7b8169bdbebb459b83603bc6794782d7",
      "f69cb279259f440eaa2d62d20f9167a0",
      "84691f9d4741475485dd93864dcfd0d5",
      "895b4e3eed92482aa56e023eb77b8537",
      "d614a9aab61648ff9d99987cd9e761d6",
      "e6f7d2ded1964835922a4f75dc647048",
      "0cb264eb581240ff9c3f7e76a505c627",
      "70258b5c961644b58bcf484b4fbac453",
      "923dd4f405474f878915a7ef6e949f22",
      "c28524e736f54032aa86e981895d7def",
      "fb6bbf902767452591434aded52cb044",
      "cb3c62cfbf664911af2aa62f2e47efc9",
      "f0e99ae75e5b4bca88fdb99b76afb8e4",
      "7d2cdf40c4ef4bdabf7a21b940cc15b3",
      "4093160305ff447eb64283d67dbdc477",
      "3ff96342094c4a8db2d4061681df4c7a",
      "c6e2b858dc564d63a97d88c628933f96",
      "b3c764edc2b74bad9168439897212fde",
      "0c88c4052e0348858c0de6841b47d72f",
      "64b4d78b460748b78bf17555de7f59d1",
      "49501af281ec47149c00b58f1c02822e",
      "deee5f6ffbde46e7bb73afaeb62d9e5e",
      "6b0fe47a2c5d436c8665c7c071996409",
      "0b1c7bbce167463eb8b2b6c5b026bb6a",
      "20c8dab6d4c24de58e42da13b6104449",
      "1724ee36e455424484eeee442dba9cbf",
      "a522400513124ddd96b5dce358f75b1f",
      "c936be45815742d3ac3575b2c2c15587",
      "3423d960b921423cb15e18446f4d47a1",
      "25b2284c62814489ae89b0860de29a57",
      "7737af1920584ca99c7d7cd5d12ad5e6",
      "d59f29bf712a499087387b92c80fb7c8",
      "e41b1fc44cbd44ed8a6adb2d65aa7dc0",
      "83e22c41e4494ef388c9ecca87494a76",
      "89dcc161a6ff4b258b14e50d48bf708a",
      "a1cd481ecaa54ac4809d24c90d8afb7f",
      "e249a464566f439ea67ec491a49d7df0",
      "d0691b1f5304438ea517c9ddbf838776",
      "cbd9fc30037b402b86571ca04a31f812",
      "c8339673b8ee40479d9cc351d0d659f0",
      "e716f953eb7348ec87168e455184be9e",
      "b2829333dd49497db6205bdc4285161f",
      "ea4d28b2be79443f8c7054afde08732f",
      "47b69c1073934fc0a8de235bc2aed3f8",
      "a0061d280b8b44f592a153121155f901",
      "2a58613ee2564578bf82fc07b23e663d",
      "9d50a0304ec94042b124cf903e5f54b9",
      "7ec77255652b49de9dd70035e0b16d63",
      "fbe4dd3163244aadbe35f39c5d609d7a",
      "d199ad2e94f9417181e1ac066f447e4d",
      "456cca30c3d8464da1ec4e0439aeed0f",
      "b9358ebbf8d34ef78b07ad7ffc327340",
      "9903b8fe2bf74bc6a65157449712089e",
      "dd8d34e194d34b709b3744e9ad5c3afb",
      "b949ae08067342c580ad065d2a010c8d",
      "821bb2af7b4649f2882a14e57d9ee25f",
      "31a7243882f649f086c249e91d244283",
      "5028821d95ae4dd3aa6a0d284b1b6a9a",
      "8446a3317fb64147a846ba3fe7b2bcb4",
      "f12cecc7d5b04a6490371fc1153751a8",
      "aa57b513e9554ea5b26642368818a50a",
      "7a945ed3980d41c6b062f6ade7141f44",
      "ed57f65eb7ef4afaa2d45e9a71c676b9",
      "82f78d2285c24a799db07edab52bbd48",
      "be62ac6df8074fce9423cc46ca6f43b3",
      "ee5c6baa9ca74cb88f67afc030fbec60",
      "90262e5ce5944cd0a41d4c0b0e88d4b9",
      "e5114c5c4441435c8df127a882b1435c",
      "51de23570d50413abab51584fa7d97b7",
      "b4f282ebcbd740538db2eb521abef433",
      "ec30bbf68626402bb5405d0e70ca8422",
      "57f694c148b649579c53c7ac36dba8cb",
      "9b348d2c534c4466ba9c45537ded8140",
      "6ae967d4834b464eaf1fa835529aea43",
      "22ba4effdae047a0926e5c5e5a35a07f",
      "883054361af547219c666f9170ae204c",
      "cce3cde24f1441b1a4d5c40353c8ebba",
      "e60a870f60e147faaeb2265e171a9ce5",
      "73f8f5ac3b074aab96666b1e0c3657c1",
      "fe183f31325d4558b3dddf2f2202a7df",
      "b37217a06f5c444b857cb021c6e81236",
      "49ccc479659e4250b4aa8dff9dea7d72",
      "74d65053fa744f1fba4a37a1aa54799e",
      "de26ef33be6d40bca957c42fc92f796c",
      "3f97eecf154d42ad8a136f356a6d6a3d",
      "59d45698e4bc4cb7bdb22c4bfc2e29ed",
      "4ab7ca9bc66d4ec9aee7882c4f9745c7",
      "982687bda6324b54936c49b3321808c9",
      "3db8f6ac18974b9dbb59fa7164591c1d",
      "e65f2f604c0e40558b833a9b13829b7b",
      "272f18f6ea6b49a387d1359d07092500",
      "ea07f87f78b34c0fbbe8634c8d152e9b",
      "155e16bbb250469fb71fc8a9243bd0f7",
      "87f23c49389341bbae8e16da23346ee4",
      "497328c5098245f493e1d084c0c462e4",
      "8eee8b24c7604764bcdc474a5abaf2a5",
      "9f47e2bcd7134436ac83602650974c02",
      "af9b14a58e9b4478acb8734b62b3afff",
      "f2991500e0e9415eab1c586ff1f42b89",
      "f8feaa10b7c64b46a0c140b81bdcd027",
      "b19aed8ef421433ab24ef33767793796",
      "675207810664456b8aa597ecdd3ea311",
      "2151e4eb8fb14cf5b5e5a8187c912f96",
      "b54f4f52640b48b0ae693db11de27062",
      "634f1fc8f3cb4d9ea3c8ee9f5f7bc34a",
      "2bcef6af99e54936a199b6906ad6f22d",
      "5e80608f5f4f477ca606d659bd01390c",
      "af97a2e4102a4127ad59f78acd861f41",
      "929055ba783f4dc19b9379988b982b64",
      "9b3f0821cd48429b832d86e3caab494f"
     ]
    },
    "id": "qshXbjXYG3bv",
    "outputId": "342afab7-55f2-4238-cf54-4ab7d6b36a82"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[3/8] Loading LLaMA 3.2 3B model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b8169bdbebb459b83603bc6794782d7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb3c62cfbf664911af2aa62f2e47efc9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0fe47a2c5d436c8665c7c071996409"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83e22c41e4494ef388c9ecca87494a76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0061d280b8b44f592a153121155f901"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "821bb2af7b4649f2882a14e57d9ee25f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90262e5ce5944cd0a41d4c0b0e88d4b9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e60a870f60e147faaeb2265e171a9ce5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3db8f6ac18974b9dbb59fa7164591c1d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8feaa10b7c64b46a0c140b81bdcd027"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Model loaded in 8-bit\n",
      "✓ Model size: ~3B parameters\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare for k-bit training\n",
    "print(\"\\n[4/8] Preparing model for QLoRA...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# (Optional but recommended) disable cache during training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5_QYXiFG5VF",
    "outputId": "f466f0f1-3d8d-4117-e252-ad796861cce1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(\"✓ LoRA adapters added\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vV9MGpaTMWs8",
    "outputId": "14e17f46-abab-4236-a44e-e0c4127128f2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ LoRA adapters added\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def format_and_tokenize(example, force=4096):\n",
    "    # Apply LLaMA chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize (NO padding, NO manual labels)\n",
    "    tokenized = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=force,   # IMPORTANT: increase for long transcripts\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return tokenized\n"
   ],
   "metadata": {
    "id": "XDK91ebsNGg9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply tokenization\n",
    "print(\"✓ Tokenizing train dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_and_tokenize,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "775da9ceb5ed4816992f4bc530fb71ab",
      "c397e30dc3714bc2bb08734bf464f2be",
      "300a7639fa514f73acaf11b3d74307d9",
      "db3fa66963164ad0a7d2a803cabadfd4",
      "cca859b763a94e7cbf450bcb29a5e682",
      "8ccb2288964c47fea31103421ef1ea4b",
      "6a4b0624de7d4bb0a26cbe88a2f348cb",
      "e1ba30eb924e4a59a0c34b0d59f9cb86",
      "73e18e3fbea547e3826d4722295d84e2",
      "d50184016a624de889476643c87b9824",
      "45877544ec8d4bee9d8d6730f7f51c97"
     ]
    },
    "id": "arkjlKIfNHr5",
    "outputId": "d07466b8-a335-42e6-f7ae-979201d5e145"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Tokenizing train dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train:   0%|          | 0/6249 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "775da9ceb5ed4816992f4bc530fb71ab"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"✓ Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    format_and_tokenize,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "db1967589fb64f1c8eaaf339f5618706",
      "58b86ead0b4f4791be95ac1ec2238420",
      "b19cf641be7c435485abc29902791e70",
      "176226d83b2e4bdca350d26c6aea4aaf",
      "beb22cc0bcfa4ac596e9240ede75a7f3",
      "f6c5200cba554acb84521b7dc40561e8",
      "95c7680d6c5d4e43baae9522631f156e",
      "95b8600c10134a4ea5defe24ccf3cf50",
      "6e0c2eacbda94a9fa71d438a8a8cb96e",
      "55d6b6dd79bf46378ac9df1485411874",
      "7d67363d746a4012add5ed8ab667dddf"
     ]
    },
    "id": "kuhKKpoCNbR2",
    "outputId": "7ece36c0-555d-4f67-9f49-46da86dcd0c3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Tokenizing validation dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db1967589fb64f1c8eaaf339f5618706"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Show sample stats (pre-training)\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\n✓ Sample stats:\")\n",
    "print(f\"  - Input length: {len(sample['input_ids'])} tokens\")\n",
    "print(f\"  - Attention tokens: {sum(sample['attention_mask'])} tokens\")\n",
    "print(f\"  - Truncated: {'Yes' if len(sample['input_ids']) == 4096 else 'No'}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgUZ-YnZNdPS",
    "outputId": "c5847139-73c4-4198-d866-5f499199b3b5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "✓ Sample stats:\n",
      "  - Input length: 1504 tokens\n",
      "  - Attention tokens: 1504 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[6/8] Configuring training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3.2-3b-qlora-summary\",\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # Memory-optimized batch settings\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Learning settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Efficiency\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CULgXFWNc_5",
    "outputId": "bfb80967-cc91-48f8-b439-e9546d1fed9e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"✓ Training configuration:\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Total training steps: ~{len(train_dataset) * 3 // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNSvpcKANyTv",
    "outputId": "da0db069-bbda-41f7-ac66-329fc630b61e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~1171\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data collator with dynamic padding\n",
    "print(\"\\n[7/8] Creating data collator...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Efficient padding for GPU\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY73Sc91N0aF",
    "outputId": "c70c8748-565c-437f-e898-f5bea9c62c49"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ],
   "metadata": {
    "id": "1J0Y1WphOHXe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "print(\"\\n[8/8] Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING IN PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "K0yxOct0OLhs",
    "outputId": "eaa92178-3f30-4a69-9f79-2e2f0dfb1ed2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 1:18:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.164900</td>\n",
       "      <td>2.286720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.145200</td>\n",
       "      <td>2.253277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.158900</td>\n",
       "      <td>2.239438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=391, training_loss=2.296681910219705, metrics={'train_runtime': 4714.9168, 'train_samples_per_second': 1.325, 'train_steps_per_second': 0.083, 'total_flos': 1.2578198791033651e+17, 'train_loss': 2.296681910219705, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(\"./llama3.2-3b-qlora-summary\")\n",
    "tokenizer.save_pretrained(\"./llama3.2-3b-qlora-summary\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi9w90hXONsX",
    "outputId": "65053a58-45e4-4e17-8d2b-fd58ea520ec6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./llama3.2-3b-qlora-summary/tokenizer_config.json',\n",
       " './llama3.2-3b-qlora-summary/special_tokens_map.json',\n",
       " './llama3.2-3b-qlora-summary/chat_template.jinja',\n",
       " './llama3.2-3b-qlora-summary/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d3d6383",
    "outputId": "6e5b1cb4-982c-4ab6-e72a-998336eff541"
   },
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: llama3.2-3b-qlora-summary/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json (deflated 76%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt (deflated 11%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json (deflated 77%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt (deflated 12%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/training_args.bin (deflated 53%)\n"
     ]
    }
   ]
  }
 ]
}
