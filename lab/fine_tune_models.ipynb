{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "Aim of this Notebook is to fine-tune the [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) with the [TL;DR Dataset](https://huggingface.co/datasets/trl-lib/tldr) and Custom Dataset and export them for evalution later.\n",
    "\n",
    "* Model_1 - Fine-Tune [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) with TL;DR first and then with the Custom Dataset\n",
    "* Model_2 - Fine-Tune [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) with Custom Dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages\n",
    "\n",
    "we would be including the packages required for the fine-tuning (as this notebook runs on colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iuzUsqiCCEiy",
    "outputId": "cd4155e3-52ff-4332-ed8c-aa7bdf644d2d"
   },
   "outputs": [],
   "source": [
    "!pip install pandas datasets\n",
    "!pip install transformers torch\n",
    "!pip install xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "id": "rdX_zO20JVq2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ccbca90d-9b9d-48e0-aab9-48893d722e2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # or Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "# we are doing this so we can import src folder\n",
    "\n",
    "from transformers import Trainer\n",
    "from src.load_dataset import load_jsonl, split_90_and_10, print_sample, TL_DR_JSON, CS_JSON\n",
    "from src.utils.torch import ensure_device\n",
    "from src.train_model import print_train_progress, configure_trainer, print_args, export_model, EXPORT_TLDR_FINE_TUNED, EXPORT_TLDR_CS_FINE_TUNED, EXPORT_CS_FINE_TUNED\n",
    "from src.load_model import load_tokenizer, load_model, lora_config_for, apply_formatter, TOKEN_LIMIT_FOR_CS, \\\n",
    "    format_dataset, prep_data_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhOjeEb4DCW9",
    "outputId": "6b0dbeb8-aeeb-46dd-e732-5e64432f2910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "source": [
    "ensure_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_1\n",
    "first we will start fine-tuning the [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) on the [TL;DR Dataset](https://huggingface.co/datasets/trl-lib/tldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyTCKMBEGZWg"
   },
   "source": [
    "## Loading the TL;DR Dataset\n",
    "\n",
    "we have saved the TL;DR Dataset in the JSONL format. we would load the `proc_tldr.jsonl` file. you can refer to this [notebook](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/export_dataset.ipynb) that generated this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsrNdOKbGLzr",
    "outputId": "d47fb7a1-82f2-412d-b5ae-0dd69c6b60e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Loading dataset...\n",
      "✓ Loaded 6944 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = load_jsonl(TL_DR_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIHDfzqiMhCm"
   },
   "source": [
    "## Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ieh8zxBGf5S",
    "outputId": "f8326cc8-7742-4fa2-e60b-f539e3d0b408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 6249 | Val: 695\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "split_dataset = split_90_and_10(dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbHyajW7Ms33"
   },
   "source": [
    "## Note\n",
    "\n",
    "we have to log in inside hugging face since the LLaMA 3.2 3B Model is a gated repository. and it requires approval from their repo. admins in order to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS_KmcPPfkc3",
    "outputId": "565c3e29-bedd-4dc6-d086-5e69489b75c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer\n",
    "\n",
    "this is where we load the [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) and its Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590,
     "referenced_widgets": [
      "7b8169bdbebb459b83603bc6794782d7",
      "f69cb279259f440eaa2d62d20f9167a0",
      "84691f9d4741475485dd93864dcfd0d5",
      "895b4e3eed92482aa56e023eb77b8537",
      "d614a9aab61648ff9d99987cd9e761d6",
      "e6f7d2ded1964835922a4f75dc647048",
      "0cb264eb581240ff9c3f7e76a505c627",
      "70258b5c961644b58bcf484b4fbac453",
      "923dd4f405474f878915a7ef6e949f22",
      "c28524e736f54032aa86e981895d7def",
      "fb6bbf902767452591434aded52cb044",
      "cb3c62cfbf664911af2aa62f2e47efc9",
      "f0e99ae75e5b4bca88fdb99b76afb8e4",
      "7d2cdf40c4ef4bdabf7a21b940cc15b3",
      "4093160305ff447eb64283d67dbdc477",
      "3ff96342094c4a8db2d4061681df4c7a",
      "c6e2b858dc564d63a97d88c628933f96",
      "b3c764edc2b74bad9168439897212fde",
      "0c88c4052e0348858c0de6841b47d72f",
      "64b4d78b460748b78bf17555de7f59d1",
      "49501af281ec47149c00b58f1c02822e",
      "deee5f6ffbde46e7bb73afaeb62d9e5e",
      "6b0fe47a2c5d436c8665c7c071996409",
      "0b1c7bbce167463eb8b2b6c5b026bb6a",
      "20c8dab6d4c24de58e42da13b6104449",
      "1724ee36e455424484eeee442dba9cbf",
      "a522400513124ddd96b5dce358f75b1f",
      "c936be45815742d3ac3575b2c2c15587",
      "3423d960b921423cb15e18446f4d47a1",
      "25b2284c62814489ae89b0860de29a57",
      "7737af1920584ca99c7d7cd5d12ad5e6",
      "d59f29bf712a499087387b92c80fb7c8",
      "e41b1fc44cbd44ed8a6adb2d65aa7dc0",
      "83e22c41e4494ef388c9ecca87494a76",
      "89dcc161a6ff4b258b14e50d48bf708a",
      "a1cd481ecaa54ac4809d24c90d8afb7f",
      "e249a464566f439ea67ec491a49d7df0",
      "d0691b1f5304438ea517c9ddbf838776",
      "cbd9fc30037b402b86571ca04a31f812",
      "c8339673b8ee40479d9cc351d0d659f0",
      "e716f953eb7348ec87168e455184be9e",
      "b2829333dd49497db6205bdc4285161f",
      "ea4d28b2be79443f8c7054afde08732f",
      "47b69c1073934fc0a8de235bc2aed3f8",
      "a0061d280b8b44f592a153121155f901",
      "2a58613ee2564578bf82fc07b23e663d",
      "9d50a0304ec94042b124cf903e5f54b9",
      "7ec77255652b49de9dd70035e0b16d63",
      "fbe4dd3163244aadbe35f39c5d609d7a",
      "d199ad2e94f9417181e1ac066f447e4d",
      "456cca30c3d8464da1ec4e0439aeed0f",
      "b9358ebbf8d34ef78b07ad7ffc327340",
      "9903b8fe2bf74bc6a65157449712089e",
      "dd8d34e194d34b709b3744e9ad5c3afb",
      "b949ae08067342c580ad065d2a010c8d",
      "821bb2af7b4649f2882a14e57d9ee25f",
      "31a7243882f649f086c249e91d244283",
      "5028821d95ae4dd3aa6a0d284b1b6a9a",
      "8446a3317fb64147a846ba3fe7b2bcb4",
      "f12cecc7d5b04a6490371fc1153751a8",
      "aa57b513e9554ea5b26642368818a50a",
      "7a945ed3980d41c6b062f6ade7141f44",
      "ed57f65eb7ef4afaa2d45e9a71c676b9",
      "82f78d2285c24a799db07edab52bbd48",
      "be62ac6df8074fce9423cc46ca6f43b3",
      "ee5c6baa9ca74cb88f67afc030fbec60",
      "90262e5ce5944cd0a41d4c0b0e88d4b9",
      "e5114c5c4441435c8df127a882b1435c",
      "51de23570d50413abab51584fa7d97b7",
      "b4f282ebcbd740538db2eb521abef433",
      "ec30bbf68626402bb5405d0e70ca8422",
      "57f694c148b649579c53c7ac36dba8cb",
      "9b348d2c534c4466ba9c45537ded8140",
      "6ae967d4834b464eaf1fa835529aea43",
      "22ba4effdae047a0926e5c5e5a35a07f",
      "883054361af547219c666f9170ae204c",
      "cce3cde24f1441b1a4d5c40353c8ebba",
      "e60a870f60e147faaeb2265e171a9ce5",
      "73f8f5ac3b074aab96666b1e0c3657c1",
      "fe183f31325d4558b3dddf2f2202a7df",
      "b37217a06f5c444b857cb021c6e81236",
      "49ccc479659e4250b4aa8dff9dea7d72",
      "74d65053fa744f1fba4a37a1aa54799e",
      "de26ef33be6d40bca957c42fc92f796c",
      "3f97eecf154d42ad8a136f356a6d6a3d",
      "59d45698e4bc4cb7bdb22c4bfc2e29ed",
      "4ab7ca9bc66d4ec9aee7882c4f9745c7",
      "982687bda6324b54936c49b3321808c9",
      "3db8f6ac18974b9dbb59fa7164591c1d",
      "e65f2f604c0e40558b833a9b13829b7b",
      "272f18f6ea6b49a387d1359d07092500",
      "ea07f87f78b34c0fbbe8634c8d152e9b",
      "155e16bbb250469fb71fc8a9243bd0f7",
      "87f23c49389341bbae8e16da23346ee4",
      "497328c5098245f493e1d084c0c462e4",
      "8eee8b24c7604764bcdc474a5abaf2a5",
      "9f47e2bcd7134436ac83602650974c02",
      "af9b14a58e9b4478acb8734b62b3afff",
      "f2991500e0e9415eab1c586ff1f42b89",
      "f8feaa10b7c64b46a0c140b81bdcd027",
      "b19aed8ef421433ab24ef33767793796",
      "675207810664456b8aa597ecdd3ea311",
      "2151e4eb8fb14cf5b5e5a8187c912f96",
      "b54f4f52640b48b0ae693db11de27062",
      "634f1fc8f3cb4d9ea3c8ee9f5f7bc34a",
      "2bcef6af99e54936a199b6906ad6f22d",
      "5e80608f5f4f477ca606d659bd01390c",
      "af97a2e4102a4127ad59f78acd861f41",
      "929055ba783f4dc19b9379988b982b64",
      "9b3f0821cd48429b832d86e3caab494f"
     ]
    },
    "id": "qshXbjXYG3bv",
    "outputId": "342afab7-55f2-4238-cf54-4ab7d6b36a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/8] Loading LLaMA 3.2 3B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8169bdbebb459b83603bc6794782d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3c62cfbf664911af2aa62f2e47efc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0fe47a2c5d436c8665c7c071996409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e22c41e4494ef388c9ecca87494a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0061d280b8b44f592a153121155f901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821bb2af7b4649f2882a14e57d9ee25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90262e5ce5944cd0a41d4c0b0e88d4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a870f60e147faaeb2265e171a9ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db8f6ac18974b9dbb59fa7164591c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8feaa10b7c64b46a0c140b81bdcd027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 8-bit\n",
      "✓ Model size: ~3B parameters\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5_QYXiFG5VF",
    "outputId": "f466f0f1-3d8d-4117-e252-ad796861cce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ],
   "source": [
    "model = lora_config_for(model) #getting model with quantized LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vV9MGpaTMWs8",
    "outputId": "14e17f46-abab-4236-a44e-e0c4127128f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LoRA adapters added\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = apply_formatter(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "775da9ceb5ed4816992f4bc530fb71ab",
      "c397e30dc3714bc2bb08734bf464f2be",
      "300a7639fa514f73acaf11b3d74307d9",
      "db3fa66963164ad0a7d2a803cabadfd4",
      "cca859b763a94e7cbf450bcb29a5e682",
      "8ccb2288964c47fea31103421ef1ea4b",
      "6a4b0624de7d4bb0a26cbe88a2f348cb",
      "e1ba30eb924e4a59a0c34b0d59f9cb86",
      "73e18e3fbea547e3826d4722295d84e2",
      "d50184016a624de889476643c87b9824",
      "45877544ec8d4bee9d8d6730f7f51c97"
     ]
    },
    "id": "arkjlKIfNHr5",
    "outputId": "d07466b8-a335-42e6-f7ae-979201d5e145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775da9ceb5ed4816992f4bc530fb71ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/6249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "db1967589fb64f1c8eaaf339f5618706",
      "58b86ead0b4f4791be95ac1ec2238420",
      "b19cf641be7c435485abc29902791e70",
      "176226d83b2e4bdca350d26c6aea4aaf",
      "beb22cc0bcfa4ac596e9240ede75a7f3",
      "f6c5200cba554acb84521b7dc40561e8",
      "95c7680d6c5d4e43baae9522631f156e",
      "95b8600c10134a4ea5defe24ccf3cf50",
      "6e0c2eacbda94a9fa71d438a8a8cb96e",
      "55d6b6dd79bf46378ac9df1485411874",
      "7d67363d746a4012add5ed8ab667dddf"
     ]
    },
    "id": "kuhKKpoCNbR2",
    "outputId": "7ece36c0-555d-4f67-9f49-46da86dcd0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1967589fb64f1c8eaaf339f5618706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgUZ-YnZNdPS",
    "outputId": "c5847139-73c4-4198-d866-5f499199b3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Sample stats:\n",
      "  - Input length: 1504 tokens\n",
      "  - Attention tokens: 1504 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ],
   "source": [
    "print_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CULgXFWNc_5",
    "outputId": "bfb80967-cc91-48f8-b439-e9546d1fed9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ],
   "source": [
    "training_args = configure_trainer(EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNSvpcKANyTv",
    "outputId": "da0db069-bbda-41f7-ac66-329fc630b61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~1171\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY73Sc91N0aF",
    "outputId": "c70c8748-565c-437f-e898-f5bea9c62c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J0Y1WphOHXe"
   },
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training _(With TL;DR DataSet - Model_1)_\n",
    "\n",
    "we have observed the run took around ~1.5 hrs to complete the run with T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "K0yxOct0OLhs",
    "outputId": "eaa92178-3f30-4a69-9f79-2e2f0dfb1ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 1:18:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.164900</td>\n",
       "      <td>2.286720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.145200</td>\n",
       "      <td>2.253277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.158900</td>\n",
       "      <td>2.239438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=391, training_loss=2.296681910219705, metrics={'train_runtime': 4714.9168, 'train_samples_per_second': 1.325, 'train_steps_per_second': 0.083, 'total_flos': 1.2578198791033651e+17, 'train_loss': 2.296681910219705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi9w90hXONsX",
    "outputId": "65053a58-45e4-4e17-8d2b-fd58ea520ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llama3.2-3b-qlora-summary/tokenizer_config.json',\n",
       " './llama3.2-3b-qlora-summary/special_tokens_map.json',\n",
       " './llama3.2-3b-qlora-summary/chat_template.jinja',\n",
       " './llama3.2-3b-qlora-summary/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d3d6383",
    "outputId": "6e5b1cb4-982c-4ab6-e72a-998336eff541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: llama3.2-3b-qlora-summary/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json (deflated 76%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt (deflated 11%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json (deflated 77%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt (deflated 12%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/training_args.bin (deflated 53%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary # zip the model for saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning Complete _(With TL;DR DataSet - Model_1)_\n",
    "\n",
    "we have successfully fine-tuned the LLaMA 3.2 3B model on the TL;DR Dataset and exported it to `./llama_3b_3_2.zip` we would have use this model and then further fine-tune with the custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuning with the Custom Dataset\n",
    "\n",
    "we have are fine-tuning the LLaMA 3.2 3B model on the Custom Dataset.\n",
    "\n",
    "### Note\n",
    "\n",
    "we have observed the fine-tuning with the Custom Dataset requires more GPU More so we have changed our run type to use A100 GPU, so we would need to load first load the Model Exported from the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3mxuyTBS55cs",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "10a1a1cc-b395-44db-b196-33ac83e6cd4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  llama_3b_3_2.zip\n",
      "   creating: llama3.2-3b-qlora-summary/\n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/special_tokens_map.json  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-300/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-391/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/training_args.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip llama_3b_3_2.zip # unzip model again for import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2rJ1Q_i7UEa"
   },
   "source": [
    "## Loading the Custom Dataset\n",
    "\n",
    "we have saved the Custom Dataset in the JSONL format. we would load the custom_dataset.jsonl file. you can refer to this [notebook](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/export_dataset.ipynb) that generated this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqWtfaR97TdF",
    "outputId": "3a80a6d2-b841-4950-c8cc-ec23318fe52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "✓ Loaded 1004 examples\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "custom_dataset = load_jsonl(CS_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o79dBf7H7csI"
   },
   "source": [
    "## Preparing the Train and the Test set\n",
    "\n",
    "Split 0.1 (90% - Train and 10% - Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYuB4DQA7dJU",
    "outputId": "3192b680-fe1e-4183-a4d3-d3cc02bf11f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 903 | Val: 101\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "split_dataset = split_90_and_10(custom_dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht_24_aQ7FnZ"
   },
   "source": [
    "## Loading the FineTuned Model\n",
    "\n",
    "Loading the Fine-tuned (with TL;DR Dataset) LLaMA 3.2 3B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JOEi8Kqm7Bxz"
   },
   "outputs": [],
   "source": [
    "# we would be loading the tokenizer from the previously fine-tuned model\n",
    "tokenizer = load_tokenizer(EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "28e288939f2c423db810e768bb665ad7",
      "c45c1fbb84f74d6caff3a351ece215e1",
      "2f19ba1837fe41dfb5f8b7d24bf2ef30",
      "fdd6947a5f3a45a898f86217340383be",
      "60352afbbc8e4ac3b06643abf42e4778",
      "9c5834c801374f0a966ca959736737ab",
      "90fe1352b94e49e9b456e856d938fe05",
      "cb2658ae97c44cdaba54120ff099bb95",
      "b5af82b2981b4b2f95c9d23e0e12281d",
      "1678597f935b4038a3995af6be8ab98f",
      "afa4d7e1f8e84c9994bbfce3e7269b7a",
      "54368211681a46969e5d35b0c486cd54",
      "c92ec1a039d74c2c8c3d420d93ea29ad",
      "c3afc0a2a36842c39324126a5a47be7a",
      "f83d710e94e9473486633b182ef42f9d",
      "929cf5fcc4c54dc9880645b02aa49019",
      "6ab75f327dd442b385a8a74fadeac732",
      "a7a3c1bca59441b59e11fc527891def5",
      "40ce3231f410402d98ff8de52bbb080c",
      "8f740f89b31a4970989b87d0136e0c10",
      "b90e0551299041cbb10e94c5e39997ec",
      "9b9b15815add4833ba4d3f9c9a0a220d",
      "c78e183297ad4aaaa2068d77585da771",
      "4b5b2a0ebf5548f7a1216cb7528e9eed",
      "e4e823f7e0534449a3098b190997b777",
      "9ee2d2b9210e43d488db5d3a521de9ab",
      "2795341f93b84596a072fff4ba5dd36f",
      "deac29310ee44debbe9950af260223dc",
      "9eab1f9c9920434d98559dc7898f521d",
      "8d9f007b65d54d82b9232f6d7ffcc650",
      "7556d705f38a4aa6803c6920030a49b8",
      "7c5b462d85bf4b918f73c550f45e4488",
      "2c91da5ae3c44187b92d10a7d5415b76",
      "41b20cefdff847339d917930a84fb599",
      "d4168edac6a647b5a98c9d59e13d0a34",
      "65a08533dc264aefa13c14d87f3d0021",
      "6aee3cd03fa140b59439e5a202b0631b",
      "db481d47ccd64bfdadedd4f6d8934234",
      "1552503fb79e4728aabe410df4a44c34",
      "0c6c497588ca414c91418521206c1960",
      "754d5aaee3ec4b31b9b8b3f4596bd4fb",
      "2ee44906562a45a1a8da0ec541d510ec",
      "3b73bc9a112a46faa748349707d7fb81",
      "ca170cacc7114ea6b73fe6a657a91347",
      "bd542cdf04d748419ceb5a80c47fdea2",
      "e3198cceed7c4ffe8c05d2b48fc28cf8",
      "d1f9d93db10b49a68ac8a1d5da58f1c0",
      "39d1a60509454418a00dc271eeec0026",
      "d53fcedfa5ff456c8c5a2b8ad4489437",
      "6312647e40834bfdb447e78fa3e0e3d7",
      "308f1d53f1f2410083ad3ca3e29ae0e8",
      "e03f946d19e74834996599b96f7b28e9",
      "904de34c41d04ae396df0b30a63ab56a",
      "ee6e1a2ca1064882a6f8de720e95d235",
      "4c8466da76624767b8e37db0ada2fc56",
      "afe9fff2f13f42cb82c28b6509ed4910",
      "eadb5bc00fb04fed9bee3353786b17f6",
      "f097e8e8d5be4df59e13decc236e655e",
      "06e17020fd5f43a7be595da8b772c128",
      "a01cdc023e67498eab0142d726b4a2b1",
      "febdc95b9b794659864c3536d9fff8f0",
      "badab8a0c1624b5eabcd0883e4983490",
      "3eca85fdb0344b829ced3e358b884f07",
      "a88cc77950684ac18fd87228b230f5b4",
      "2c61ee56737349c3a460261a3ed996e4",
      "41e6e20b60b941158d791dce1ae4d66f",
      "29943391e97c4ceda2303b3a3ff5e10e",
      "3c2aeff0e7dd4b09b90ccf8e14b35d82",
      "e72dcf11b45e4597a19acae2a018ecb9",
      "8fdff50a4500452d8fdd312d6d0a6cac",
      "9cd46db95f8d467fb99ce035d0718a2a",
      "3b22e67d1b5f42f586bc566deefa864f",
      "49529c3b03184c59bbcf68d0f9ecbf59",
      "a1a5a2aaaa19430a8a23a8e56d685c6c",
      "42214c1e3b1b474882de330e51a373f3",
      "25d22fead85540c985146f9fa470d48c",
      "e4b998eebc8e4066b95e874faebf74ec"
     ]
    },
    "id": "Agl1d9Bi7FRu",
    "outputId": "6f87266d-83a3-47ef-81ae-22681d36535a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e288939f2c423db810e768bb665ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54368211681a46969e5d35b0c486cd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78e183297ad4aaaa2068d77585da771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b20cefdff847339d917930a84fb599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd542cdf04d748419ceb5a80c47fdea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe9fff2f13f42cb82c28b6509ed4910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29943391e97c4ceda2303b3a3ff5e10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fine-tuned model loaded\n"
     ]
    }
   ],
   "source": [
    "# first we load the base model\n",
    "model = load_model()\n",
    "model = lora_config_for(model, EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0LnD0tW8dvC",
    "outputId": "406b787e-b21e-42d1-e6c6-43b9944ed151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZcXKahaNVOT",
    "outputId": "d19476bf-dc23-400d-9011-fefb773b4210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path='meta-llama/Llama-3.2-3B-Instruct', revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config  # for verifying lora config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx1qqkZr80nU"
   },
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset (Youtube Transcripts), since the number of tokens in the custom dataset can be significantly larger than the TL;DR Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDNnjkHY8qe3"
   },
   "outputs": [],
   "source": [
    "formatter = apply_formatter(tokenizer, token_limit=TOKEN_LIMIT_FOR_CS) # get formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "53d8d239999e469cb6850103e801c864",
      "0b83dbdc6a19463ea90fcc42eea54b11",
      "decfc913b3004159aa075e07fa583246",
      "9a3d7e7c7d3b40acbe27918bf82dffb4",
      "3801984aae9a44679d69089ad521b643",
      "e1677499cfe94deea12371b6aae59fa4",
      "cc7733e013164e7d933217651afd789a",
      "7bde5cc3262c45d282dcba39e2259c2f",
      "c9e2a5ed59d84ec396b50b793e57d461",
      "dfe50d9480174ab3947ce61f30ad6d3e",
      "8877d3829953482b8301d61674e9a623"
     ]
    },
    "id": "roVseVT688BG",
    "outputId": "a532758b-30f0-4b0f-aed4-b65f922e5042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d8d239999e469cb6850103e801c864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "7e8ebf414bc84468aa8dfe5cd769c109",
      "d6ada58d30384f828ee6f2b114aa9fdf",
      "85fe0a43d986433687ca5fc28b3a945e",
      "dac79eedb7074358bb856b12b01b0729",
      "d74fd5cd20144bb7b5c56170d67ae79d",
      "9e7684a87a484b468a6d3095ddc4293a",
      "2773706e56574a77a3b173eb3fd3710d",
      "489be62562dc4e9bb557f0061c977cf1",
      "f78e71fb215543e1b1f76d41d6b88318",
      "70447924f7474642a6ee5e64f750004c",
      "ec8f94304244458bbf73b90d678c6e30"
     ]
    },
    "id": "BXof1zja88O6",
    "outputId": "4d5550c5-f7eb-45e8-83a7-7ee8f34496fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8ebf414bc84468aa8dfe5cd769c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYYvowak9DRx",
    "outputId": "32a04f82-c70b-466d-ff71-71327fb9e711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Sample stats:\n",
      "  - Input length: 1382 tokens\n",
      "  - Attention tokens: 1382 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ],
   "source": [
    "# Show sample stats (pre-training)\n",
    "print_sample(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPZgzWLy9Eww",
    "outputId": "c967dad4-8f27-4e6d-c71b-bca0dfcd2bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs is 3 with the custom dataset\n",
    "training_args = configure_trainer(EXPORT_TLDR_CS_FINE_TUNED, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzKAZpYU9GX8",
    "outputId": "3038088e-7689-4926-83c2-903c58521c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~169\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG_BRUVo9II8",
    "outputId": "46a260dc-d3bc-4cc8-e2b7-d04ebbe0d109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer) #init data collatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z8tofMzP9JcZ"
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (With Custom DataSet - Model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "lktCWD8F9K0-",
    "outputId": "72fe1a62-8948-4876-a061-2cc1ec5e5edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 1:51:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.998300</td>\n",
       "      <td>2.143472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=2.1060634607460065, metrics={'train_runtime': 6778.3477, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.025, 'total_flos': 2.0566782399297946e+17, 'train_loss': 2.1060634607460065, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT_CZX7E9MTa",
    "outputId": "c67636ed-9ea4-40fb-eeb6-178abf1a543f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final-summary/tokenizer_config.json',\n",
       " './final-summary/special_tokens_map.json',\n",
       " './final-summary/chat_template.jinja',\n",
       " './final-summary/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_TLDR_CS_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgb769MhBbWc",
    "outputId": "9e3b363f-da44-4080-f7aa-6cabf8e4665d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: final-summary/ (stored 0%)\n",
      "  adding: final-summary/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-100/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-100/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-100/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-100/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-100/trainer_state.json (deflated 70%)\n",
      "  adding: final-summary/checkpoint-100/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-100/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-100/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-100/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-171/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-171/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-171/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-171/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-171/trainer_state.json (deflated 73%)\n",
      "  adding: final-summary/checkpoint-171/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-171/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-171/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-171/adapter_model.safetensors (deflated 8%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/tokenizer.json (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r ./final-summary.zip ./final-summary # zip model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Completed\n",
    "\n",
    "we have observed the fine tune with the custom dataset took ~3 hrs to complete the run with A100 GPU. and we have exported it to `./final-summary.zip`\n",
    "\n",
    "**we now have Model_1 which is fine-tuned on the TL;DR Dataset and then on the Custom Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Model_2\n",
    "\n",
    "Here we would fine-tuning the LLAMA 3.2 3B model on the Custom Dataset only.\n",
    "\n",
    "Since we saw the Custom Dataset requires more GPU, we would be using A100 GPU for this run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing\n",
    "\n",
    "\n",
    "(Doing this again since python's gc would have collected old ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "split_dataset = split_90_and_10(custom_dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer & Model\n",
    "\n",
    "we would be loading base model (LLMA 3.2 3B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring LoRA for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8f310dd6309d904",
    "outputId": "198a15f5-0bf8-4af0-9f14-dc25e783db40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = lora_config_for(model) # get lora config again for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "296c236a9d6a2e47"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e24b38aa050ff81e"
   },
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset (YouTube Transcripts), since the number of tokens in the custom dataset can be significantly larger than the TL;DR Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8d70cada36e9678",
    "outputId": "7b03e785-ea11-40a0-9386-b0f8cddecfb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 903 | Val: 101\n"
     ]
    }
   ],
   "source": [
    "formatter = apply_formatter(tokenizer, token_limit=TOKEN_LIMIT_FOR_CS) # get formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample stats (pre-training)\n",
    "print_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs is 3 with the custom dataset\n",
    "training_args = configure_trainer(EXPORT_CS_FINE_TUNED, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a310c7619d5fb446",
    "outputId": "cb0d9473-6d4d-46b4-8ca6-f82eb31cfe39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/8] Configuring 8-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b309b94758c63a53",
    "outputId": "0c2e9c7e-0fbe-485a-cebb-9dde176158b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training\n",
    "\n",
    "This is the Final Fine tune we have observed that this took again ~3hrs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6b118b8c71b8ac2a"
   },
   "outputs": [],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8417378e9c246156"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_CS_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.071580300Z",
     "start_time": "2025-12-16T18:03:09.043362700Z"
    },
    "id": "ade5964302e23d6f"
   },
   "outputs": [],
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.098147400Z",
     "start_time": "2025-12-16T18:03:09.076244700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "40721eadaef549548f87001ecea35dcd",
      "9b87f908a6e742549c5abd38eecf0ddc",
      "08a5bdcac39f4740843ff5e5fd6e59d8",
      "1870187756f6437091654a3218ed3459",
      "5f23470c9b3c4e9aa1b9c2c3c23ac0da",
      "e917ca409a754e089557548ec76379f2",
      "608ed57f73174073b7750aa12fa6bd07",
      "ac4a6f95cfe741428e0bac707fb68e00",
      "80654e1791244ad3ab9a42924878017c",
      "dd0af92267ff49a2bea6e39cb3709d50",
      "675203004af54aea87b794f3625741d7"
     ]
    },
    "id": "9c8323333f3edffa",
    "outputId": "90132092-c0f4-4d9e-c1e5-b27dc9eed9e6"
   },
   "source": [
    "# Completed\n",
    "---\n",
    "* Fine-tuned the **Llama 3.2 3B** model in two stages:\n",
    "  * First, on the `TL;DR` dataset.\n",
    "  * Second, on a custom dataset.\n",
    "* Fine-tuned the **Llama 3.2 3B** model with only custom dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
