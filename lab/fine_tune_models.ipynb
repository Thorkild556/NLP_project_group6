{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "Aim of this Notebook is to fine-tune the [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) with the [TL;DR Dataset](https://huggingface.co/datasets/trl-lib/tldr) and Custom Dataset and export them for evalution later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Packages\n",
    "\n",
    "we would be including the packages required for the fine-tuning (as this notebook runs on colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iuzUsqiCCEiy",
    "outputId": "cd4155e3-52ff-4332-ed8c-aa7bdf644d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3Gp8_uFAIa0u",
    "outputId": "312ad154-845a-4ce1-e2f6-aa583e8a9c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jsBLvi_EG_fr",
    "outputId": "72b6f71f-cd42-4863-d650-e5f456a6ec3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Collecting torch==2.9.1 (from xformers)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->xformers)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.9/122.9 MB\u001B[0m \u001B[31m19.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m47.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.0/88.0 MB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m954.8/954.8 kB\u001B[0m \u001B[31m41.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.1/193.1 MB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m63.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.6/63.6 MB\u001B[0m \u001B[31m30.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.5/267.5 MB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m288.2/288.2 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.3/39.3 MB\u001B[0m \u001B[31m50.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.5/170.5 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.26.1-py3-none-any.whl (517 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m517.4/517.4 kB\u001B[0m \u001B[31m43.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.1/59.1 MB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, bitsandbytes, trl\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1 trl-0.26.1 xformers-0.0.33.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": true,
    "id": "rdX_zO20JVq2",
    "outputId": "ccbca90d-9b9d-48e0-aab9-48893d722e2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # or Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "# we are doing this so we can import src folder\n",
    "\n",
    "from transformers import Trainer\n",
    "from src.load_dataset import load_jsonl, split, print_sample, TL_DR_JSON, CS_JSON\n",
    "from src.utils.torch import ensure_device\n",
    "from src.train_model import print_train_progress, configure_trainer, print_args, export_model, EXPORT_TLDR_FINE_TUNED, EXPORT_TLDR_CS_FINE_TUNED, EXPORT_CS_FINE_TUNED\n",
    "from src.load_model import load_tokenizer, load_model, lora_config_for, apply_formatter, TOKEN_LIMIT_FOR_CS, \\\n",
    "    format_dataset, prep_data_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhOjeEb4DCW9",
    "outputId": "6b0dbeb8-aeeb-46dd-e732-5e64432f2910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "source": [
    "ensure_device()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model_1\n",
    "first we will start fine-tuning the [LLaMA 3.2 3B Instruct model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) on the [TL;DR Dataset](https://huggingface.co/datasets/trl-lib/tldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyTCKMBEGZWg"
   },
   "source": [
    "## Loading the TL;DR Dataset\n",
    "\n",
    "we have saved the TL;DR Dataset in the JSONL format. we would load the `proc_tldr.jsonl` file. you can refer to this [notebook](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/export_dataset.ipynb) that generated this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsrNdOKbGLzr",
    "outputId": "d47fb7a1-82f2-412d-b5ae-0dd69c6b60e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/8] Loading dataset...\n",
      "✓ Loaded 6944 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = load_jsonl(TL_DR_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIHDfzqiMhCm"
   },
   "source": [
    "## Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ieh8zxBGf5S",
    "outputId": "f8326cc8-7742-4fa2-e60b-f539e3d0b408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 6249 | Val: 695\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "split_dataset = split(dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbHyajW7Ms33"
   },
   "source": [
    "## Note\n",
    "\n",
    "we have to log in inside hugging face since the LLaMA 3.2 3B Model is a gated repository. and it requires approval from their repo. admins in order to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS_KmcPPfkc3",
    "outputId": "565c3e29-bedd-4dc6-d086-5e69489b75c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001B[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model and Tokenizer\n",
    "\n",
    "this is where we load the LLMALaMA 3.2 3B Instruct's Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590,
     "referenced_widgets": [
      "7b8169bdbebb459b83603bc6794782d7",
      "f69cb279259f440eaa2d62d20f9167a0",
      "84691f9d4741475485dd93864dcfd0d5",
      "895b4e3eed92482aa56e023eb77b8537",
      "d614a9aab61648ff9d99987cd9e761d6",
      "e6f7d2ded1964835922a4f75dc647048",
      "0cb264eb581240ff9c3f7e76a505c627",
      "70258b5c961644b58bcf484b4fbac453",
      "923dd4f405474f878915a7ef6e949f22",
      "c28524e736f54032aa86e981895d7def",
      "fb6bbf902767452591434aded52cb044",
      "cb3c62cfbf664911af2aa62f2e47efc9",
      "f0e99ae75e5b4bca88fdb99b76afb8e4",
      "7d2cdf40c4ef4bdabf7a21b940cc15b3",
      "4093160305ff447eb64283d67dbdc477",
      "3ff96342094c4a8db2d4061681df4c7a",
      "c6e2b858dc564d63a97d88c628933f96",
      "b3c764edc2b74bad9168439897212fde",
      "0c88c4052e0348858c0de6841b47d72f",
      "64b4d78b460748b78bf17555de7f59d1",
      "49501af281ec47149c00b58f1c02822e",
      "deee5f6ffbde46e7bb73afaeb62d9e5e",
      "6b0fe47a2c5d436c8665c7c071996409",
      "0b1c7bbce167463eb8b2b6c5b026bb6a",
      "20c8dab6d4c24de58e42da13b6104449",
      "1724ee36e455424484eeee442dba9cbf",
      "a522400513124ddd96b5dce358f75b1f",
      "c936be45815742d3ac3575b2c2c15587",
      "3423d960b921423cb15e18446f4d47a1",
      "25b2284c62814489ae89b0860de29a57",
      "7737af1920584ca99c7d7cd5d12ad5e6",
      "d59f29bf712a499087387b92c80fb7c8",
      "e41b1fc44cbd44ed8a6adb2d65aa7dc0",
      "83e22c41e4494ef388c9ecca87494a76",
      "89dcc161a6ff4b258b14e50d48bf708a",
      "a1cd481ecaa54ac4809d24c90d8afb7f",
      "e249a464566f439ea67ec491a49d7df0",
      "d0691b1f5304438ea517c9ddbf838776",
      "cbd9fc30037b402b86571ca04a31f812",
      "c8339673b8ee40479d9cc351d0d659f0",
      "e716f953eb7348ec87168e455184be9e",
      "b2829333dd49497db6205bdc4285161f",
      "ea4d28b2be79443f8c7054afde08732f",
      "47b69c1073934fc0a8de235bc2aed3f8",
      "a0061d280b8b44f592a153121155f901",
      "2a58613ee2564578bf82fc07b23e663d",
      "9d50a0304ec94042b124cf903e5f54b9",
      "7ec77255652b49de9dd70035e0b16d63",
      "fbe4dd3163244aadbe35f39c5d609d7a",
      "d199ad2e94f9417181e1ac066f447e4d",
      "456cca30c3d8464da1ec4e0439aeed0f",
      "b9358ebbf8d34ef78b07ad7ffc327340",
      "9903b8fe2bf74bc6a65157449712089e",
      "dd8d34e194d34b709b3744e9ad5c3afb",
      "b949ae08067342c580ad065d2a010c8d",
      "821bb2af7b4649f2882a14e57d9ee25f",
      "31a7243882f649f086c249e91d244283",
      "5028821d95ae4dd3aa6a0d284b1b6a9a",
      "8446a3317fb64147a846ba3fe7b2bcb4",
      "f12cecc7d5b04a6490371fc1153751a8",
      "aa57b513e9554ea5b26642368818a50a",
      "7a945ed3980d41c6b062f6ade7141f44",
      "ed57f65eb7ef4afaa2d45e9a71c676b9",
      "82f78d2285c24a799db07edab52bbd48",
      "be62ac6df8074fce9423cc46ca6f43b3",
      "ee5c6baa9ca74cb88f67afc030fbec60",
      "90262e5ce5944cd0a41d4c0b0e88d4b9",
      "e5114c5c4441435c8df127a882b1435c",
      "51de23570d50413abab51584fa7d97b7",
      "b4f282ebcbd740538db2eb521abef433",
      "ec30bbf68626402bb5405d0e70ca8422",
      "57f694c148b649579c53c7ac36dba8cb",
      "9b348d2c534c4466ba9c45537ded8140",
      "6ae967d4834b464eaf1fa835529aea43",
      "22ba4effdae047a0926e5c5e5a35a07f",
      "883054361af547219c666f9170ae204c",
      "cce3cde24f1441b1a4d5c40353c8ebba",
      "e60a870f60e147faaeb2265e171a9ce5",
      "73f8f5ac3b074aab96666b1e0c3657c1",
      "fe183f31325d4558b3dddf2f2202a7df",
      "b37217a06f5c444b857cb021c6e81236",
      "49ccc479659e4250b4aa8dff9dea7d72",
      "74d65053fa744f1fba4a37a1aa54799e",
      "de26ef33be6d40bca957c42fc92f796c",
      "3f97eecf154d42ad8a136f356a6d6a3d",
      "59d45698e4bc4cb7bdb22c4bfc2e29ed",
      "4ab7ca9bc66d4ec9aee7882c4f9745c7",
      "982687bda6324b54936c49b3321808c9",
      "3db8f6ac18974b9dbb59fa7164591c1d",
      "e65f2f604c0e40558b833a9b13829b7b",
      "272f18f6ea6b49a387d1359d07092500",
      "ea07f87f78b34c0fbbe8634c8d152e9b",
      "155e16bbb250469fb71fc8a9243bd0f7",
      "87f23c49389341bbae8e16da23346ee4",
      "497328c5098245f493e1d084c0c462e4",
      "8eee8b24c7604764bcdc474a5abaf2a5",
      "9f47e2bcd7134436ac83602650974c02",
      "af9b14a58e9b4478acb8734b62b3afff",
      "f2991500e0e9415eab1c586ff1f42b89",
      "f8feaa10b7c64b46a0c140b81bdcd027",
      "b19aed8ef421433ab24ef33767793796",
      "675207810664456b8aa597ecdd3ea311",
      "2151e4eb8fb14cf5b5e5a8187c912f96",
      "b54f4f52640b48b0ae693db11de27062",
      "634f1fc8f3cb4d9ea3c8ee9f5f7bc34a",
      "2bcef6af99e54936a199b6906ad6f22d",
      "5e80608f5f4f477ca606d659bd01390c",
      "af97a2e4102a4127ad59f78acd861f41",
      "929055ba783f4dc19b9379988b982b64",
      "9b3f0821cd48429b832d86e3caab494f"
     ]
    },
    "id": "qshXbjXYG3bv",
    "outputId": "342afab7-55f2-4238-cf54-4ab7d6b36a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/8] Loading LLaMA 3.2 3B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8169bdbebb459b83603bc6794782d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3c62cfbf664911af2aa62f2e47efc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0fe47a2c5d436c8665c7c071996409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e22c41e4494ef388c9ecca87494a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0061d280b8b44f592a153121155f901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821bb2af7b4649f2882a14e57d9ee25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90262e5ce5944cd0a41d4c0b0e88d4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a870f60e147faaeb2265e171a9ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db8f6ac18974b9dbb59fa7164591c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8feaa10b7c64b46a0c140b81bdcd027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 8-bit\n",
      "✓ Model size: ~3B parameters\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5_QYXiFG5VF",
    "outputId": "f466f0f1-3d8d-4117-e252-ad796861cce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ],
   "source": [
    "model = lora_config_for(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vV9MGpaTMWs8",
    "outputId": "14e17f46-abab-4236-a44e-e0c4127128f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LoRA adapters added\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = apply_formatter(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "775da9ceb5ed4816992f4bc530fb71ab",
      "c397e30dc3714bc2bb08734bf464f2be",
      "300a7639fa514f73acaf11b3d74307d9",
      "db3fa66963164ad0a7d2a803cabadfd4",
      "cca859b763a94e7cbf450bcb29a5e682",
      "8ccb2288964c47fea31103421ef1ea4b",
      "6a4b0624de7d4bb0a26cbe88a2f348cb",
      "e1ba30eb924e4a59a0c34b0d59f9cb86",
      "73e18e3fbea547e3826d4722295d84e2",
      "d50184016a624de889476643c87b9824",
      "45877544ec8d4bee9d8d6730f7f51c97"
     ]
    },
    "id": "arkjlKIfNHr5",
    "outputId": "d07466b8-a335-42e6-f7ae-979201d5e145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775da9ceb5ed4816992f4bc530fb71ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/6249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "db1967589fb64f1c8eaaf339f5618706",
      "58b86ead0b4f4791be95ac1ec2238420",
      "b19cf641be7c435485abc29902791e70",
      "176226d83b2e4bdca350d26c6aea4aaf",
      "beb22cc0bcfa4ac596e9240ede75a7f3",
      "f6c5200cba554acb84521b7dc40561e8",
      "95c7680d6c5d4e43baae9522631f156e",
      "95b8600c10134a4ea5defe24ccf3cf50",
      "6e0c2eacbda94a9fa71d438a8a8cb96e",
      "55d6b6dd79bf46378ac9df1485411874",
      "7d67363d746a4012add5ed8ab667dddf"
     ]
    },
    "id": "kuhKKpoCNbR2",
    "outputId": "7ece36c0-555d-4f67-9f49-46da86dcd0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1967589fb64f1c8eaaf339f5618706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgUZ-YnZNdPS",
    "outputId": "c5847139-73c4-4198-d866-5f499199b3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Sample stats:\n",
      "  - Input length: 1504 tokens\n",
      "  - Attention tokens: 1504 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ],
   "source": [
    "print_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CULgXFWNc_5",
    "outputId": "bfb80967-cc91-48f8-b439-e9546d1fed9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ],
   "source": [
    "training_args = configure_trainer(EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNSvpcKANyTv",
    "outputId": "da0db069-bbda-41f7-ac66-329fc630b61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~1171\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eY73Sc91N0aF",
    "outputId": "c70c8748-565c-437f-e898-f5bea9c62c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J0Y1WphOHXe"
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training _(With TL;DR DataSet - Model_1)_\n",
    "\n",
    "we have observed the run took around ~1.5 hrs to complete the run with T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "K0yxOct0OLhs",
    "outputId": "eaa92178-3f30-4a69-9f79-2e2f0dfb1ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 1:18:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.164900</td>\n",
       "      <td>2.286720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.145200</td>\n",
       "      <td>2.253277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.158900</td>\n",
       "      <td>2.239438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=391, training_loss=2.296681910219705, metrics={'train_runtime': 4714.9168, 'train_samples_per_second': 1.325, 'train_steps_per_second': 0.083, 'total_flos': 1.2578198791033651e+17, 'train_loss': 2.296681910219705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi9w90hXONsX",
    "outputId": "65053a58-45e4-4e17-8d2b-fd58ea520ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llama3.2-3b-qlora-summary/tokenizer_config.json',\n",
       " './llama3.2-3b-qlora-summary/special_tokens_map.json',\n",
       " './llama3.2-3b-qlora-summary/chat_template.jinja',\n",
       " './llama3.2-3b-qlora-summary/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d3d6383",
    "outputId": "6e5b1cb4-982c-4ab6-e72a-998336eff541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: llama3.2-3b-qlora-summary/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json (deflated 76%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt (deflated 11%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt (deflated 61%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json (deflated 77%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt (deflated 12%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/training_args.bin (deflated 53%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning Complete _(With TL;DR DataSet - Model_1)_\n",
    "\n",
    "we have successfully fine-tuned the LLaMA 3.2 3B model on the TL;DR Dataset and exported it to `./llama_3b_3_2.zip` we would have use this model and then further fine-tune with the custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FineTuning with the Custom Dataset\n",
    "\n",
    "we have are fine-tuning the LLaMA 3.2 3B model on the Custom Dataset.\n",
    "\n",
    "### Note\n",
    "\n",
    "we have observed the fine-tuning with the Custom Dataset requires more GPU More so we have changed our run type to use A100 GPU, so we would need to load first load the Model Exported from the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3mxuyTBS55cs",
    "outputId": "10a1a1cc-b395-44db-b196-33ac83e6cd4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  llama_3b_3_2.zip\n",
      "   creating: llama3.2-3b-qlora-summary/\n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/special_tokens_map.json  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-300/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-391/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/training_args.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip llama_3b_3_2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2rJ1Q_i7UEa"
   },
   "source": [
    "## Loading the Custom Dataset\n",
    "\n",
    "we have saved the Custom Dataset in the JSONL format. we would load the custom_dataset.jsonl file. you can refer to this [notebook](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/export_dataset.ipynb) that generated this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqWtfaR97TdF",
    "outputId": "3a80a6d2-b841-4950-c8cc-ec23318fe52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "✓ Loaded 1004 examples\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "custom_dataset = load_jsonl(CS_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o79dBf7H7csI"
   },
   "source": [
    "## Preparing the Train and the Test set\n",
    "\n",
    "Split 0.1 (90% - Train and 10% - Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYuB4DQA7dJU",
    "outputId": "3192b680-fe1e-4183-a4d3-d3cc02bf11f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 903 | Val: 101\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "split_dataset = split(custom_dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ht_24_aQ7FnZ"
   },
   "source": [
    "## Loading the FineTuned Model\n",
    "\n",
    "Loading the Fine-tuned (with TL;DR Dataset) LLaMA 3.2 3B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JOEi8Kqm7Bxz"
   },
   "outputs": [],
   "source": [
    "# we would be loading the tokenizer from the previously fine-tuned model\n",
    "tokenizer = load_tokenizer(EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "28e288939f2c423db810e768bb665ad7",
      "c45c1fbb84f74d6caff3a351ece215e1",
      "2f19ba1837fe41dfb5f8b7d24bf2ef30",
      "fdd6947a5f3a45a898f86217340383be",
      "60352afbbc8e4ac3b06643abf42e4778",
      "9c5834c801374f0a966ca959736737ab",
      "90fe1352b94e49e9b456e856d938fe05",
      "cb2658ae97c44cdaba54120ff099bb95",
      "b5af82b2981b4b2f95c9d23e0e12281d",
      "1678597f935b4038a3995af6be8ab98f",
      "afa4d7e1f8e84c9994bbfce3e7269b7a",
      "54368211681a46969e5d35b0c486cd54",
      "c92ec1a039d74c2c8c3d420d93ea29ad",
      "c3afc0a2a36842c39324126a5a47be7a",
      "f83d710e94e9473486633b182ef42f9d",
      "929cf5fcc4c54dc9880645b02aa49019",
      "6ab75f327dd442b385a8a74fadeac732",
      "a7a3c1bca59441b59e11fc527891def5",
      "40ce3231f410402d98ff8de52bbb080c",
      "8f740f89b31a4970989b87d0136e0c10",
      "b90e0551299041cbb10e94c5e39997ec",
      "9b9b15815add4833ba4d3f9c9a0a220d",
      "c78e183297ad4aaaa2068d77585da771",
      "4b5b2a0ebf5548f7a1216cb7528e9eed",
      "e4e823f7e0534449a3098b190997b777",
      "9ee2d2b9210e43d488db5d3a521de9ab",
      "2795341f93b84596a072fff4ba5dd36f",
      "deac29310ee44debbe9950af260223dc",
      "9eab1f9c9920434d98559dc7898f521d",
      "8d9f007b65d54d82b9232f6d7ffcc650",
      "7556d705f38a4aa6803c6920030a49b8",
      "7c5b462d85bf4b918f73c550f45e4488",
      "2c91da5ae3c44187b92d10a7d5415b76",
      "41b20cefdff847339d917930a84fb599",
      "d4168edac6a647b5a98c9d59e13d0a34",
      "65a08533dc264aefa13c14d87f3d0021",
      "6aee3cd03fa140b59439e5a202b0631b",
      "db481d47ccd64bfdadedd4f6d8934234",
      "1552503fb79e4728aabe410df4a44c34",
      "0c6c497588ca414c91418521206c1960",
      "754d5aaee3ec4b31b9b8b3f4596bd4fb",
      "2ee44906562a45a1a8da0ec541d510ec",
      "3b73bc9a112a46faa748349707d7fb81",
      "ca170cacc7114ea6b73fe6a657a91347",
      "bd542cdf04d748419ceb5a80c47fdea2",
      "e3198cceed7c4ffe8c05d2b48fc28cf8",
      "d1f9d93db10b49a68ac8a1d5da58f1c0",
      "39d1a60509454418a00dc271eeec0026",
      "d53fcedfa5ff456c8c5a2b8ad4489437",
      "6312647e40834bfdb447e78fa3e0e3d7",
      "308f1d53f1f2410083ad3ca3e29ae0e8",
      "e03f946d19e74834996599b96f7b28e9",
      "904de34c41d04ae396df0b30a63ab56a",
      "ee6e1a2ca1064882a6f8de720e95d235",
      "4c8466da76624767b8e37db0ada2fc56",
      "afe9fff2f13f42cb82c28b6509ed4910",
      "eadb5bc00fb04fed9bee3353786b17f6",
      "f097e8e8d5be4df59e13decc236e655e",
      "06e17020fd5f43a7be595da8b772c128",
      "a01cdc023e67498eab0142d726b4a2b1",
      "febdc95b9b794659864c3536d9fff8f0",
      "badab8a0c1624b5eabcd0883e4983490",
      "3eca85fdb0344b829ced3e358b884f07",
      "a88cc77950684ac18fd87228b230f5b4",
      "2c61ee56737349c3a460261a3ed996e4",
      "41e6e20b60b941158d791dce1ae4d66f",
      "29943391e97c4ceda2303b3a3ff5e10e",
      "3c2aeff0e7dd4b09b90ccf8e14b35d82",
      "e72dcf11b45e4597a19acae2a018ecb9",
      "8fdff50a4500452d8fdd312d6d0a6cac",
      "9cd46db95f8d467fb99ce035d0718a2a",
      "3b22e67d1b5f42f586bc566deefa864f",
      "49529c3b03184c59bbcf68d0f9ecbf59",
      "a1a5a2aaaa19430a8a23a8e56d685c6c",
      "42214c1e3b1b474882de330e51a373f3",
      "25d22fead85540c985146f9fa470d48c",
      "e4b998eebc8e4066b95e874faebf74ec"
     ]
    },
    "id": "Agl1d9Bi7FRu",
    "outputId": "6f87266d-83a3-47ef-81ae-22681d36535a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e288939f2c423db810e768bb665ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54368211681a46969e5d35b0c486cd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78e183297ad4aaaa2068d77585da771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b20cefdff847339d917930a84fb599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd542cdf04d748419ceb5a80c47fdea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe9fff2f13f42cb82c28b6509ed4910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29943391e97c4ceda2303b3a3ff5e10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fine-tuned model loaded\n"
     ]
    }
   ],
   "source": [
    "# first we load the base model\n",
    "model = load_model()\n",
    "model = lora_config_for(model, EXPORT_TLDR_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0LnD0tW8dvC",
    "outputId": "406b787e-b21e-42d1-e6c6-43b9944ed151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZcXKahaNVOT",
    "outputId": "d19476bf-dc23-400d-9011-fefb773b4210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path='meta-llama/Llama-3.2-3B-Instruct', revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config  # for verifying lora config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx1qqkZr80nU"
   },
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset (Youtube Transcripts), since the number of tokens in the custom dataset can be significantly larger than the TL;DR Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yDNnjkHY8qe3"
   },
   "outputs": [],
   "source": [
    "formatter = apply_formatter(tokenizer, token_limit=TOKEN_LIMIT_FOR_CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "53d8d239999e469cb6850103e801c864",
      "0b83dbdc6a19463ea90fcc42eea54b11",
      "decfc913b3004159aa075e07fa583246",
      "9a3d7e7c7d3b40acbe27918bf82dffb4",
      "3801984aae9a44679d69089ad521b643",
      "e1677499cfe94deea12371b6aae59fa4",
      "cc7733e013164e7d933217651afd789a",
      "7bde5cc3262c45d282dcba39e2259c2f",
      "c9e2a5ed59d84ec396b50b793e57d461",
      "dfe50d9480174ab3947ce61f30ad6d3e",
      "8877d3829953482b8301d61674e9a623"
     ]
    },
    "id": "roVseVT688BG",
    "outputId": "a532758b-30f0-4b0f-aed4-b65f922e5042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d8d239999e469cb6850103e801c864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "7e8ebf414bc84468aa8dfe5cd769c109",
      "d6ada58d30384f828ee6f2b114aa9fdf",
      "85fe0a43d986433687ca5fc28b3a945e",
      "dac79eedb7074358bb856b12b01b0729",
      "d74fd5cd20144bb7b5c56170d67ae79d",
      "9e7684a87a484b468a6d3095ddc4293a",
      "2773706e56574a77a3b173eb3fd3710d",
      "489be62562dc4e9bb557f0061c977cf1",
      "f78e71fb215543e1b1f76d41d6b88318",
      "70447924f7474642a6ee5e64f750004c",
      "ec8f94304244458bbf73b90d678c6e30"
     ]
    },
    "id": "BXof1zja88O6",
    "outputId": "4d5550c5-f7eb-45e8-83a7-7ee8f34496fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizing validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8ebf414bc84468aa8dfe5cd769c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYYvowak9DRx",
    "outputId": "32a04f82-c70b-466d-ff71-71327fb9e711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Sample stats:\n",
      "  - Input length: 1382 tokens\n",
      "  - Attention tokens: 1382 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ],
   "source": [
    "# Show sample stats (pre-training)\n",
    "print_sample(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPZgzWLy9Eww",
    "outputId": "c967dad4-8f27-4e6d-c71b-bca0dfcd2bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs is 3 with the custom dataset\n",
    "training_args = configure_trainer(EXPORT_TLDR_CS_FINE_TUNED, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzKAZpYU9GX8",
    "outputId": "3038088e-7689-4926-83c2-903c58521c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~169\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DG_BRUVo9II8",
    "outputId": "46a260dc-d3bc-4cc8-e2b7-d04ebbe0d109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z8tofMzP9JcZ"
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training (With Custom DataSet - Model_1)"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "lktCWD8F9K0-",
    "outputId": "72fe1a62-8948-4876-a061-2cc1ec5e5edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 1:51:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.998300</td>\n",
       "      <td>2.143472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=2.1060634607460065, metrics={'train_runtime': 6778.3477, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.025, 'total_flos': 2.0566782399297946e+17, 'train_loss': 2.1060634607460065, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT_CZX7E9MTa",
    "outputId": "c67636ed-9ea4-40fb-eeb6-178abf1a543f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final-summary/tokenizer_config.json',\n",
       " './final-summary/special_tokens_map.json',\n",
       " './final-summary/chat_template.jinja',\n",
       " './final-summary/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_TLDR_CS_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgb769MhBbWc",
    "outputId": "9e3b363f-da44-4080-f7aa-6cabf8e4665d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: final-summary/ (stored 0%)\n",
      "  adding: final-summary/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-100/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-100/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-100/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-100/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-100/trainer_state.json (deflated 70%)\n",
      "  adding: final-summary/checkpoint-100/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-100/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-100/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-100/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-171/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-171/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-171/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-171/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-171/trainer_state.json (deflated 73%)\n",
      "  adding: final-summary/checkpoint-171/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-171/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-171/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-171/adapter_model.safetensors (deflated 8%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/tokenizer.json (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r ./final-summary.zip ./final-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Completed\n",
    "\n",
    "we have observed the fine tune with the custom dataset took ~3 hrs to complete the run with A100 GPU. and we have exported it to `./final-summary.zip`\n",
    "\n",
    "**we now have Model_1 which is fine-tuned on the TL;DR Dataset and then on the Custom Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Model_2\n",
    "\n",
    "Here we would fine-tuning the LLAMA 3.2 3B model on the Custom Dataset only.\n",
    "\n",
    "Since we saw the Custom Dataset requires more GPU, we would be using A100 GPU for this run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing\n",
    "\n",
    "\n",
    "(Doing this again since python's gc would have collected old ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "split_dataset = split(custom_dataset)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer & Model\n",
    "\n",
    "we would be loading base model (LLMA 3.2 3B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring LoRA for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8f310dd6309d904",
    "outputId": "198a15f5-0bf8-4af0-9f14-dc25e783db40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = lora_config_for(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "296c236a9d6a2e47"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e24b38aa050ff81e"
   },
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset (YouTube Transcripts), since the number of tokens in the custom dataset can be significantly larger than the TL;DR Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8d70cada36e9678",
    "outputId": "7b03e785-ea11-40a0-9386-b0f8cddecfb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train: 903 | Val: 101\n"
     ]
    }
   ],
   "source": [
    "formatter = apply_formatter(tokenizer, token_limit=TOKEN_LIMIT_FOR_CS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization for train dataset\n",
    "train_dataset = format_dataset(formatter, train_dataset, \"Tokenizing train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization for validation dataset\n",
    "val_dataset = format_dataset(formatter, val_dataset, \"Tokenizing validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample stats (pre-training)\n",
    "print_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs is 3 with the custom dataset\n",
    "training_args = configure_trainer(EXPORT_CS_FINE_TUNED, num_train_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_args(train_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a310c7619d5fb446",
    "outputId": "cb0d9473-6d4d-46b4-8ca6-f82eb31cfe39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/8] Configuring 8-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "data_collator = prep_data_collector(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b309b94758c63a53",
    "outputId": "0c2e9c7e-0fbe-485a-cebb-9dde176158b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Training\n",
    "\n",
    "This is the Final Fine tune we have observed that this took again ~3hrs to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6b118b8c71b8ac2a"
   },
   "outputs": [],
   "source": [
    "print_train_progress(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8417378e9c246156"
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "export_model(trainer, tokenizer, EXPORT_CS_FINE_TUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.071580300Z",
     "start_time": "2025-12-16T18:03:09.043362700Z"
    },
    "id": "ade5964302e23d6f"
   },
   "outputs": [],
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.098147400Z",
     "start_time": "2025-12-16T18:03:09.076244700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "40721eadaef549548f87001ecea35dcd",
      "9b87f908a6e742549c5abd38eecf0ddc",
      "08a5bdcac39f4740843ff5e5fd6e59d8",
      "1870187756f6437091654a3218ed3459",
      "5f23470c9b3c4e9aa1b9c2c3c23ac0da",
      "e917ca409a754e089557548ec76379f2",
      "608ed57f73174073b7750aa12fa6bd07",
      "ac4a6f95cfe741428e0bac707fb68e00",
      "80654e1791244ad3ab9a42924878017c",
      "dd0af92267ff49a2bea6e39cb3709d50",
      "675203004af54aea87b794f3625741d7"
     ]
    },
    "id": "9c8323333f3edffa",
    "outputId": "90132092-c0f4-4d9e-c1e5-b27dc9eed9e6"
   },
   "source": [
    "# Completed\n",
    "---\n",
    "* Fine-tuned the **Llama 3.2 3B** model in two stages:\n",
    "  * First, on the `TL;DR` dataset.\n",
    "  * Second, on a custom dataset.\n",
    "* Fine-tuned the **Llama 3.2 3B** model with only custom dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
