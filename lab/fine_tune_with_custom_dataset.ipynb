{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "ae91fc0f-06e0-4036-b6d4-120922de4332"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas datasets"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ada6ff246eb7586d",
    "outputId": "a1f8c2e4-083a-4232-8a3c-27ed0bc504bf"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "!pip install transformers torch"
   ],
   "id": "ada6ff246eb7586d"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2372ae28c16fc3b4",
    "outputId": "c765736a-bf5a-4bc2-9523-45a6554ff3da"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xformers in /usr/local/lib/python3.12/dist-packages (0.0.33.post2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.26.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.1 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.9.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.5.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ],
   "id": "2372ae28c16fc3b4"
  },
  {
   "metadata": {
    "id": "adad89df7ec153b"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch"
   ],
   "id": "adad89df7ec153b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8f310dd6309d904",
    "outputId": "198a15f5-0bf8-4af0-9f14-dc25e783db40"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"We would be using this device:\", device)"
   ],
   "id": "f8f310dd6309d904"
  },
  {
   "metadata": {
    "id": "296c236a9d6a2e47"
   },
   "cell_type": "markdown",
   "source": [
    "# Loading Dataset"
   ],
   "id": "296c236a9d6a2e47"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42b8ae4fe4f70311",
    "outputId": "37aa44b9-3130-4d7d-ce18-bcc057573fd6"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "\u2713 Loaded 1004 examples\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "print(\"\\n2-[1/8] Loading dataset...\")\n",
    "data = []\n",
    "with open(\"custom_dataset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "custom_dataset = Dataset.from_dict({\n",
    "    \"messages\": [item[\"messages\"] for item in data]\n",
    "})\n",
    "\n",
    "print(f\"\u2713 Loaded {len(custom_dataset)} examples\")"
   ],
   "id": "42b8ae4fe4f70311"
  },
  {
   "metadata": {
    "id": "e24b38aa050ff81e"
   },
   "cell_type": "markdown",
   "source": [
    "# Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing"
   ],
   "id": "e24b38aa050ff81e"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8d70cada36e9678",
    "outputId": "7b03e785-ea11-40a0-9386-b0f8cddecfb7"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Train: 903 | Val: 101\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "# Split dataset\n",
    "split_dataset = custom_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "print(f\"\u2713 Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ],
   "id": "d8d70cada36e9678"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a310c7619d5fb446",
    "outputId": "cb0d9473-6d4d-46b4-8ca6-f82eb31cfe39"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[2/8] Configuring 8-bit quantization...\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": [
    "print(\"\\n[2/8] Configuring 8-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "a310c7619d5fb446"
  },
  {
   "metadata": {
    "id": "ba6b21343a150d3b"
   },
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## Note\n",
    "\n",
    "we have login inside hugging face and make sure we request for access to use the LLaMA 3.2 3B model"
   ],
   "id": "ba6b21343a150d3b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "300b2c43cb2ae8c9",
    "outputId": "06e2efd8-5485-47c5-d8bc-52208fb427e1"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33m\u26a0\ufe0f  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `hf auth whoami` to get more information or `hf auth logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "!huggingface-cli login"
   ],
   "id": "300b2c43cb2ae8c9"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "b42f38f59ab24dc191639c7731e757c6",
      "d5f280aeaa6c470faa6a9350cee8634b",
      "618c5be424c54b7e8c1fa6cca884f98e",
      "18892fdd97c6401e95f82b3c33099b1d",
      "b9d5c42cfa3c483bb2350d176be56440",
      "e6edc89a460e4fc1bfcec8a29b0a03c4",
      "056246b705024ec382b35724f2e73b59",
      "92061e40281e49719b83b4e58f1a69a8",
      "096636ca917d481e9f01ecec6e467ecb",
      "718178785d3846dc94b67f3191699db3",
      "ff5712a07cd546ba8b19c8050ae23ada"
     ]
    },
    "id": "816b6dbb67a3f406",
    "outputId": "4ff87694-0597-44f0-ed3c-fe2433e2eb20"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[3/8] Loading LLaMA 3.2 3B model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b42f38f59ab24dc191639c7731e757c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Model loaded in 8-bit\n",
      "\u2713 Model size: ~3B parameters\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "# Load model and tokenizer\n",
    "print(\"\\n[3/8] Loading LLaMA 3.2 3B model...\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# LLaMA models usually do not have a pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"\u2713 Model loaded in 8-bit\")\n",
    "print(\"\u2713 Model size: ~3B parameters\")"
   ],
   "id": "816b6dbb67a3f406"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b309b94758c63a53",
    "outputId": "0c2e9c7e-0fbe-485a-cebb-9dde176158b9"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[4/8] Preparing model for QLoRA...\n"
     ]
    }
   ],
   "execution_count": 15,
   "source": [
    "# Prepare for k-bit training\n",
    "print(\"\\n[4/8] Preparing model for QLoRA...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# (Optional but recommended) disable cache during training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "id": "b309b94758c63a53"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23bf65410c3353da",
    "outputId": "f85a1e39-c898-4614-fc37-2a3269434cbf"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 LoRA adapters added\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\u2713 LoRA adapters added\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ],
   "id": "23bf65410c3353da"
  },
  {
   "metadata": {
    "id": "6b118b8c71b8ac2a"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "def format_and_tokenize(example, force=4096):\n",
    "    # Apply LLaMA chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize (NO padding, NO manual labels)\n",
    "    tokenized = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=force,   # IMPORTANT: increase for long transcripts\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return tokenized\n"
   ],
   "id": "6b118b8c71b8ac2a"
  },
  {
   "metadata": {
    "id": "8417378e9c246156"
   },
   "cell_type": "markdown",
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset"
   ],
   "id": "8417378e9c246156"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.071580300Z",
     "start_time": "2025-12-16T18:03:09.043362700Z"
    },
    "id": "ade5964302e23d6f"
   },
   "cell_type": "code",
   "source": [
    "format_and_tokenize_for_cds = lambda x: format_and_tokenize(x, force=10_000)"
   ],
   "id": "ade5964302e23d6f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.098147400Z",
     "start_time": "2025-12-16T18:03:09.076244700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "40721eadaef549548f87001ecea35dcd",
      "9b87f908a6e742549c5abd38eecf0ddc",
      "08a5bdcac39f4740843ff5e5fd6e59d8",
      "1870187756f6437091654a3218ed3459",
      "5f23470c9b3c4e9aa1b9c2c3c23ac0da",
      "e917ca409a754e089557548ec76379f2",
      "608ed57f73174073b7750aa12fa6bd07",
      "ac4a6f95cfe741428e0bac707fb68e00",
      "80654e1791244ad3ab9a42924878017c",
      "dd0af92267ff49a2bea6e39cb3709d50",
      "675203004af54aea87b794f3625741d7"
     ]
    },
    "id": "9c8323333f3edffa",
    "outputId": "90132092-c0f4-4d9e-c1e5-b27dc9eed9e6"
   },
   "cell_type": "code",
   "source": [
    "# Apply tokenization\n",
    "print(\"\u2713 Tokenizing train dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n"
   ],
   "id": "9c8323333f3edffa",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenizing train dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40721eadaef549548f87001ecea35dcd"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "f28fd941fbf94c0f86375bb8757b171a",
      "552d73bfd62b4002a26eadfe6cf0cfe7",
      "52251c4a920b4a0e90ba2a85ac769e01",
      "bad4a81cc145470eb6dda5bd177280a9",
      "0b14a8a9ab8241bb85583bf496668c28",
      "f2356314f32d4a13bb6c8e39d49e4132",
      "1af4804c4fa3408cbf186e068fb137b4",
      "cdb21dcd112a4178a65902eae7d334d7",
      "d693abafb87b4e618f0a37ea4a42f2d3",
      "767712a8e20749369d91e69a2bd0e16f",
      "4b76f736405349f8a6fd5a23e993c25c"
     ]
    },
    "id": "58b20211ea99f57b",
    "outputId": "4eef4aaa-f60c-46dc-be9c-5978ca16f6a8"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenizing validation dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f28fd941fbf94c0f86375bb8757b171a"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 20,
   "source": [
    "print(\"\u2713 Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")"
   ],
   "id": "58b20211ea99f57b"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "874017a44748d9b1",
    "outputId": "9250ee75-c867-483e-c335-2a583887d1f1"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2713 Sample stats:\n",
      "  - Input length: 1382 tokens\n",
      "  - Attention tokens: 1382 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "# Show sample stats (pre-training)\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\n\u2713 Sample stats:\")\n",
    "print(f\"  - Input length: {len(sample['input_ids'])} tokens\")\n",
    "print(f\"  - Attention tokens: {sum(sample['attention_mask'])} tokens\")\n",
    "print(f\"  - Truncated: {'Yes' if len(sample['input_ids']) == 2 ** 15 else 'No'}\")\n"
   ],
   "id": "874017a44748d9b1"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b8722b1358c4743",
    "outputId": "457a34d3-730b-44e7-9c50-f486d4a7ce4a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ],
   "execution_count": 22,
   "source": [
    "print(\"\\n[6/8] Configuring training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3.2-3b-qlora-summary\",\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # Memory-optimized batch settings\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Learning settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Efficiency\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ],
   "id": "7b8722b1358c4743"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f810b8f30717975",
    "outputId": "b54006b3-8356-4821-ce58-2b63de4ef225"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~169\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ],
   "execution_count": 23,
   "source": [
    "print(\"\u2713 Training configuration:\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Total training steps: ~{len(train_dataset) * 3 // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ],
   "id": "6f810b8f30717975"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e348a54ee3b86e5",
    "outputId": "c8dbf711-de04-4093-cb20-98d6ed97146a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ],
   "execution_count": 24,
   "source": [
    "# Data collator with dynamic padding\n",
    "print(\"\\n[7/8] Creating data collator...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Efficient padding for GPU\n",
    ")"
   ],
   "id": "e348a54ee3b86e5"
  },
  {
   "metadata": {
    "id": "a0771aa53dfc6c93"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 25,
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ],
   "id": "a0771aa53dfc6c93"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "d85a329ba6398351",
    "outputId": "8bb10c11-bff1-4117-bef3-1fa9ed02c990"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 1:50:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.013100</td>\n",
       "      <td>2.152477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=2.1440364199075086, metrics={'train_runtime': 6746.9018, 'train_samples_per_second': 0.402, 'train_steps_per_second': 0.025, 'total_flos': 2.0566782399297946e+17, 'train_loss': 2.1440364199075086, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "execution_count": 26,
   "source": [
    "# Start training\n",
    "print(\"\\n[8/8] Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING IN PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "d85a329ba6398351"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c558ddb67925379",
    "outputId": "9886052d-02a7-4522-d37e-27687d783459"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./llama3.2-3b-qlora-summary/tokenizer_config.json',\n",
       " './llama3.2-3b-qlora-summary/special_tokens_map.json',\n",
       " './llama3.2-3b-qlora-summary/chat_template.jinja',\n",
       " './llama3.2-3b-qlora-summary/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "execution_count": 27,
   "source": [
    "# Save final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(\"./llama3.2-3b-qlora-summary\")\n",
    "tokenizer.save_pretrained(\"./llama3.2-3b-qlora-summary\")"
   ],
   "id": "9c558ddb67925379"
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r ./llama_3b_3_2.zip ./llama3.2-3b-qlora-summary"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6TstxM4H5m2",
    "outputId": "8c132237-55dd-4b1f-fa12-a169381ed3ae"
   },
   "id": "O6TstxM4H5m2",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: llama3.2-3b-qlora-summary/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/trainer_state.json (deflated 69%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/optimizer.pt (deflated 11%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/scheduler.pt (deflated 62%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/ (stored 0%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/README.md (deflated 65%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/adapter_config.json (deflated 58%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/training_args.bin (deflated 53%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/rng_state.pth (deflated 26%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/trainer_state.json (deflated 72%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/optimizer.pt (deflated 11%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/scheduler.pt (deflated 62%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer.json (deflated 85%)\n",
      "  adding: llama3.2-3b-qlora-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3.2-3b-qlora-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: llama3.2-3b-qlora-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: llama3.2-3b-qlora-summary/tokenizer.json (deflated 85%)\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}