{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AIM\n",
    "\n",
    "AIM of this Notebook is to fetch the results of our test dataset from the Base Model, Models we fine-tuned.\n",
    "\n",
    "## Future Purpose\n",
    "Once saved we will compare the results and see how our fine-tuning has improved the model. (Not in this Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nlG_0Y4mJZL",
    "outputId": "00bd43bb-0990-4ec7-f12f-f3ad21b4a577"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  fine_tuned_with_cs.zip\n",
      "   creating: llama3.2-3b-qlora-summary/\n",
      "  inflating: llama3.2-3b-qlora-summary/README.md  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-100/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/training_args.bin  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-171/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip fine_tuned_with_cs.zip"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip final-summary.zip"
   ],
   "metadata": {
    "id": "Zz6x3ul0m5-F",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "030f88f4-3caa-4e1a-b57b-bc5315288363"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  final-summary.zip\n",
      "   creating: final-summary/\n",
      "  inflating: final-summary/README.md  \n",
      "   creating: final-summary/checkpoint-100/\n",
      "  inflating: final-summary/checkpoint-100/README.md  \n",
      "  inflating: final-summary/checkpoint-100/adapter_config.json  \n",
      "  inflating: final-summary/checkpoint-100/training_args.bin  \n",
      "  inflating: final-summary/checkpoint-100/special_tokens_map.json  \n",
      "  inflating: final-summary/checkpoint-100/tokenizer_config.json  \n",
      "  inflating: final-summary/checkpoint-100/rng_state.pth  \n",
      "  inflating: final-summary/checkpoint-100/trainer_state.json  \n",
      "  inflating: final-summary/checkpoint-100/chat_template.jinja  \n",
      "  inflating: final-summary/checkpoint-100/optimizer.pt  \n",
      "  inflating: final-summary/checkpoint-100/scheduler.pt  \n",
      "  inflating: final-summary/checkpoint-100/adapter_model.safetensors  \n",
      "  inflating: final-summary/checkpoint-100/tokenizer.json  \n",
      "  inflating: final-summary/adapter_config.json  \n",
      "  inflating: final-summary/training_args.bin  \n",
      "   creating: final-summary/checkpoint-171/\n",
      "  inflating: final-summary/checkpoint-171/README.md  \n",
      "  inflating: final-summary/checkpoint-171/adapter_config.json  \n",
      "  inflating: final-summary/checkpoint-171/training_args.bin  \n",
      "  inflating: final-summary/checkpoint-171/special_tokens_map.json  \n",
      "  inflating: final-summary/checkpoint-171/tokenizer_config.json  \n",
      "  inflating: final-summary/checkpoint-171/rng_state.pth  \n",
      "  inflating: final-summary/checkpoint-171/trainer_state.json  \n",
      "  inflating: final-summary/checkpoint-171/chat_template.jinja  \n",
      "  inflating: final-summary/checkpoint-171/optimizer.pt  \n",
      "  inflating: final-summary/checkpoint-171/scheduler.pt  \n",
      "  inflating: final-summary/checkpoint-171/adapter_model.safetensors  \n",
      "  inflating: final-summary/checkpoint-171/tokenizer.json  \n",
      "  inflating: final-summary/special_tokens_map.json  \n",
      "  inflating: final-summary/tokenizer_config.json  \n",
      "  inflating: final-summary/chat_template.jinja  \n",
      "  inflating: final-summary/adapter_model.safetensors  \n",
      "  inflating: final-summary/tokenizer.json  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e3k7193nCKs",
    "outputId": "5dcc04cc-d96f-4bb3-fda3-6030f903c91c"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers torch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBUjeex7nC-l",
    "outputId": "197d658d-7198-4728-c099-df757be60b41"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8doJAbBgnEqG",
    "outputId": "f647b768-9f64-486f-f9d2-1e64cdf66f8a"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Collecting torch==2.9.1 (from xformers)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->xformers)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.9/122.9 MB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m139.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.0/88.0 MB\u001B[0m \u001B[31m25.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m954.8/954.8 kB\u001B[0m \u001B[31m63.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.1/193.1 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m72.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.6/63.6 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.5/267.5 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/c2/f5/e1854cb2f2bcd4280c44736c93550cc300ff4b8c95ebe370d0aa7d2b473d/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001B[0m\u001B[33m\n",
      "\u001B[0mDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m288.2/288.2 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.3/39.3 MB\u001B[0m \u001B[31m54.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.5/170.5 MB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m518.9/518.9 kB\u001B[0m \u001B[31m43.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.1/59.1 MB\u001B[0m \u001B[31m39.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, bitsandbytes, trl\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1 trl-0.26.2 xformers-0.0.33.post2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import torch\n",
    "from gc import collect\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "id": "7WR1sJS1nF6D"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"We would be using this device:\", device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRrDs_f6na6D",
    "outputId": "dfc7a953-a647-4b06-a46c-9dc2006f826a"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "print(\"\\n2-[1/8] Loading dataset...\")\n",
    "data = []\n",
    "with open(\"custom_dataset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "custom_dataset = Dataset.from_dict({\n",
    "    \"messages\": [item[\"messages\"] for item in data]\n",
    "})\n",
    "split_dataset = custom_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"✓ Loaded {len(val_dataset)} examples\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeIfjgbnnd1R",
    "outputId": "c5984b28-81c0-4f46-824a-ae0012daaffc"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "✓ Loaded 101 examples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_msg_for_base_model(_msg):\n",
    "    messages = _msg[\"messages\"]\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"You are an AI assistant that helps provide the summary for a youtube video given we provide you with a user search text, and based on the search text youtube has given out 4 relevant videos as a result, we will give you the search text, video title and the transcript of all the result videos (it could be between 1 to 4 ), your job is to summarize the video according to the prompt and also address the similarities and differences in the video's opinion over the user's query.\n",
    "\n",
    "for example:\n",
    "\n",
    "SEARCH_TEXT: Whats the Best thing to do on Holidays\n",
    "\n",
    "VIDEO_1_TITLE: Top 5 things to do on Holidays\n",
    "VIDEO_1_TRANSCRIPT: One of the Thing to do on Holiday is to visit your grandparents and spend some time with your family....\n",
    "\n",
    "VIDEO_2_TITLE: Top 5 Games to try on Holidays\n",
    "VIDEO_2_TRANSCRIPT: Well you can try Death Stranding or Far Cry (Entire Series) so that u have time to play and also see how they grow over time and you won't see how time passes so quickly...\n",
    "\n",
    "VIDEO_3_TITLE: How to Convince your boss to work on Holidays\n",
    "VIDEO_3_TRANSCRIPT: Tells ways to work on Holidays instead of Enjoying...\n",
    "\n",
    "I am expecting Output like...\n",
    "\n",
    "Key points from Video_1:\n",
    "...\n",
    "\n",
    "Key points from Video_2:\n",
    "...\n",
    "\n",
    "Transcript 2 unpacked:\n",
    "...\n",
    "\n",
    "well Video 1 and Video 2 agree on the fact that you should spend some time in leisure during holiday be it playing games or with your family. Video 1 seems to be giving good advice and Suggestions from Video 2 can also be considered if you are into games, while Video 3 out of no where suggesting things that might not be average thing to do on Holidays, it focuses more on Working than Enjoying as opposed to  other videos\n",
    "\n",
    "\n",
    "That's the Example, please make sure to follow these rules as well.\n",
    "- No bullet points, no lists, no headings.\n",
    "- Do not use any characters that cannot be typed directly on a standard keyboard, including but not limited to em-dashes (-), en-dashes (-), smart quotes (\" \", ' '), or any other special typographic symbols. Use only standard ASCII characters.\n",
    "- You may also compare videos, for example: \"video 1 is similar to video 2 except for this part where they disagree\", or \"video 3 is more similar to video 1 where the speakers have the same views on this...\".\n",
    "- Combine the relevant points from the transcripts into smooth, continuous paragraph(s) for each video.\n",
    "- Ignore irrelevant or repeated transcript content.\n",
    "- Tone should be clear, helpful, and natural.\n",
    "- Always reference videos by their index like VIDEO_1, VIDEO_2.\n",
    "- For each video, write a short section summarizing only the key insights. Keep it 3–5 sentences or around 50–100 words.\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": messages[1][\"content\"]\n",
    "            }\n",
    "        ]\n",
    "    }"
   ],
   "metadata": {
    "id": "H5kxtmdUnl32"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "base_generation_inputs = val_dataset.map(extract_msg_for_base_model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e246372933c94db8a63bfb7117549a02",
      "83626bd183674f1abb0bb2e0a271eec4",
      "77ae32ce5a2f47e0bdf800d04af5dbc4",
      "01511b6c2ef44f4f9415dd13e7fba65d",
      "ac340b8566614c93b7209e06f427135d",
      "d2f357bb5f60476da2c1da71fb18117d",
      "7f60902bd40948c99d2ca74e770c562b",
      "bd319f8283f94806abe665391dc2eeef",
      "0f17a701f1fb4ca68dfbb95c18bff948",
      "b253649bc1fb44119f1a65dcb0ded250",
      "9463590d7be34d4fbb9f511035f194af"
     ]
    },
    "id": "luCjvuLNnuan",
    "outputId": "02b297f4-ecb0-4230-bf2f-49147a110a70"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e246372933c94db8a63bfb7117549a02"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\""
   ],
   "metadata": {
    "id": "err3vImBnwE-"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    )"
   ],
   "metadata": {
    "id": "xigzAQ5inz3_"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!huggingface-cli login"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o22n7Z60oIa6",
    "outputId": "66a101ed-3884-44e3-b5ca-4098d5850d50"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001B[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_1 Results\n",
    "\n",
    "Results from the Base Model (LLMA 3.2 3B Instruct)"
   ],
   "metadata": {
    "id": "mtV48h77BYuB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        use_fast=True\n",
    "    )\n",
    "# LLaMA models usually do not have a pad token by default\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607,
     "referenced_widgets": [
      "ca7a197568a844f2adf04b6b25f6b534",
      "4134cf11b062487cbb0df5edd6db1db3",
      "18e130a630c74b3f93e27f839b2d60b0",
      "85caca4fed2c4ef3a8ebc65e636aaa48",
      "96d0212dbd904de5b4ea538ebf77cb88",
      "b8050793ad9248b0acb3e41c4e4197d6",
      "1bda686a167a4e7bb8d100fe6d3db42c",
      "33c739194f554f24b993a56d222143d5",
      "3de04e2042de4ce09f87951e4a6fabf0",
      "68c5262aefe744fc9024aa605482e329",
      "c84f9f33439c42b89ab945934c14bf3b",
      "9b4b490954ab43b3b70cb01c194a6b0d",
      "873ff0c9d5794779bb5ac454459219cc",
      "7223c811502841a8953c986e50a1e468",
      "90988ea8ac93476c8124f78de38ae7a5",
      "37ea064da30a4cbd8148d559e24f3769",
      "4b9884ddf04e441d90db00b8ca2726b8",
      "2c171fe7ed4e47af8b68b24dfc28c582",
      "f38fa0bdb4284c25abfd6c8bbeda4226",
      "b1287d91d7034345b9f5f07e7b2ee0e0",
      "4fb854600d734388a9c112244f9ed68b",
      "eab50cb542ee443380efb9f64bd2d3ef",
      "33fc78b5f4da4c189531078ce4d08203",
      "7b4e1d1eeaac486290ee99cbd50e74c2",
      "52748c86132a41cca782f7c0ac419b13",
      "3b57e3b760cc42ec97bade8924372760",
      "257fe6e46f9b49649779b9799e65354f",
      "806e867443c34473a4c53368fae50862",
      "e664cbbee2b5499583c7c58e1ac3376c",
      "e2c84fcd35294aef86ab2a599bd19d4a",
      "359a1261a9cc48c68defa46d703f0d58",
      "4234f387ccb540d194e9e4c21e35d846",
      "0e75fb3486834667b9cba8071c971a17"
     ]
    },
    "id": "Gix46p4fn827",
    "outputId": "ae2a9629-b111-47ea-916a-c49f987ce4a3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "base_model.eval()\n",
    "base_outputs = []\n",
    "for base_msg in base_generation_inputs:\n",
    "    prompt = base_tokenizer.apply_chat_template(\n",
    "        base_msg[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = base_tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=base_tokenizer.model_max_length,\n",
    "    ).to(base_model.device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10_000,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=base_tokenizer.eos_token_id,\n",
    "            pad_token_id=base_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response = base_tokenizer.decode(\n",
    "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    base_outputs.append((base_msg, response))\n",
    "    print(len(response))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oAfePBtFqK_c",
    "outputId": "aa6a2985-9aea-4096-ead9-ffb041a399da"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4472\n",
      "2173\n",
      "2588\n",
      "3493\n",
      "1333\n",
      "7101\n",
      "3200\n",
      "2747\n",
      "3450\n",
      "2684\n",
      "2382\n",
      "5195\n",
      "3245\n",
      "2329\n",
      "3437\n",
      "1777\n",
      "3072\n",
      "1929\n",
      "2397\n",
      "3252\n",
      "2971\n",
      "3297\n",
      "2854\n",
      "2745\n",
      "2402\n",
      "3726\n",
      "1992\n",
      "2768\n",
      "1412\n",
      "3116\n",
      "3023\n",
      "2304\n",
      "5197\n",
      "3501\n",
      "3816\n",
      "2858\n",
      "2933\n",
      "1648\n",
      "2644\n",
      "3583\n",
      "3513\n",
      "2842\n",
      "2248\n",
      "2041\n",
      "2094\n",
      "1816\n",
      "4051\n",
      "1813\n",
      "2608\n",
      "3987\n",
      "2073\n",
      "2224\n",
      "2359\n",
      "2120\n",
      "3774\n",
      "3856\n",
      "3009\n",
      "2167\n",
      "3069\n",
      "2287\n",
      "3755\n",
      "3099\n",
      "2327\n",
      "4028\n",
      "2822\n",
      "2137\n",
      "2387\n",
      "3402\n",
      "1843\n",
      "2294\n",
      "2069\n",
      "2056\n",
      "3564\n",
      "6023\n",
      "2666\n",
      "5656\n",
      "3596\n",
      "2184\n",
      "2093\n",
      "4498\n",
      "2338\n",
      "2207\n",
      "1648\n",
      "5847\n",
      "3027\n",
      "3232\n",
      "5577\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(base_outputs)"
   ],
   "metadata": {
    "id": "Fcrsh3S7tWfK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Path(\"base_outputs.json\").write_text(json.dumps(base_outputs))"
   ],
   "metadata": {
    "id": "aS2flugWtWUL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "del base_model\n",
    "del base_tokenizer\n",
    "collect()"
   ],
   "metadata": {
    "id": "SAThiTv7tV-K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_2 Results\n",
    "\n",
    "Results from the Model which was Fine Tuned with only the Custom Dataset"
   ],
   "metadata": {
    "id": "AuDDewizBhtg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ADAPTER_PATH = \"./llama3.2-3b-qlora-summary\""
   ],
   "metadata": {
    "id": "b0N_yJYVuIJ6"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def split_msgs(_msg):\n",
    "    messages = _msg[\"messages\"]\n",
    "    return {\n",
    "        \"messages\": messages[: 2]\n",
    "    }"
   ],
   "metadata": {
    "id": "_C6Axpem6u-9"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generation_inputs = val_dataset.map(split_msgs)"
   ],
   "metadata": {
    "id": "uEjUfcyr6nd3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "371c50340cf4441e9f489790f906e76e",
      "49aed83ef0d34d88bc8d1bf90ddd2df0",
      "6e7bcb603eba4840bef1355462da7f21",
      "82f5c1f4893d4d758fa22824aa43bf6e",
      "d8e47bcc42f341a882c03240d2c93d76",
      "dd9fc1f01515430391e19df9b58612c6",
      "6979a74373fc4967a94c70ce351e04e3",
      "5f2a0a376b414764aed2267ad360060e",
      "7b91558969f04a25928078b96353892a",
      "612fe551a77e4c7caffafe81089324f2",
      "337dac6b92114f99b8b70aa4996932a5"
     ]
    },
    "outputId": "2a5c0e28-407d-469d-ba31-f9f1374a71da"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "371c50340cf4441e9f489790f906e76e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cs_tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "\n",
    "cs_tokenizer.pad_token = cs_tokenizer.eos_token\n",
    "\n",
    "cs_tokenizer.padding_side = \"right\"\n",
    "\n",
    "cs_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,  # SAME as original\n",
    ")\n",
    "\n",
    "cs_model = prepare_model_for_kbit_training(cs_model, use_gradient_checkpointing=True)\n",
    "cs_model = PeftModel.from_pretrained(\n",
    "    cs_model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=True   # to continue training\n",
    ")\n"
   ],
   "metadata": {
    "id": "lsD3ZcnutQXs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "f9f043557e0f4a84a47891ad77170528",
      "051c6c7e1ccb4fe88b83e092ab4fbe21",
      "852995dc0a064e88afed862598c574fc",
      "a82b867afca4466e8aa939ff9f783e9b",
      "3c9d85a77e2846a7a7da95e97eb7cbde",
      "fea9fb84b553492bbb39e73bf80d03f1",
      "833183f39ed5445381f5b89297d7d52c",
      "524cabdab23d424ebff70f9c5cd5491e",
      "6d1f0517e85d4c83ac10762dbafaec4a",
      "592432bea4814f5595a12c10dcc80d19",
      "65256af0db6e413d9c7362af34541166",
      "32130cefb1094fd4b453d7137882244b",
      "38e827ca6bd3401f811c23dc16bc9a1e",
      "6cf033cbd057462da489c189cc20a6dc",
      "7a493593a4f84e0a84587f2df117301c",
      "ad7b8df1d8df4bcf9eba4f4a70a77449",
      "ba2378c746894a4783667b939e07f84b",
      "a10d4028ea7f42bc803b465c6289fc5e",
      "716e56e32ce24b2186e71345c4072f22",
      "df12d61aab5b4df9959eb3d33b2242d0",
      "fad5a8bffb5c4961a4cd1c6534756351",
      "8fd9260c0d2c43c2ba93019179d75a08",
      "be7662a7eaee4f4080e258f287640db3",
      "f0d7aa4dbf734425b2cd191a9e48b225",
      "0a813b46adb44bec9b6b88e90e736ca5",
      "e5cacb0b49384e308d524b9eb1a51c70",
      "b3f52afc1a314a9e8d12a51c1d17f143",
      "e079f7d49754476e86ca670099d18bc5",
      "b315027d5d83445a85fca7712d80f0df",
      "991995c8e7184783b660df270062ff52",
      "fe65fb9e3d9a41039b5055fb5335c5d3",
      "e457cdfb297149779401a6eed7a46c9a",
      "6874813e81fc49c0abf6ca926903667c",
      "0b7183245ef043ca85a66e7c397722b1",
      "46f916ee0be64de3b948d99d067e7a00",
      "c80ca335dec74768b5722cb629380740",
      "d70e5f2d02954f00aff05b0242a44b1c",
      "d35b3cecf18c4950bc362372b5c40564",
      "09ee8ae67fde4916bd65a925c54a0eb9",
      "834bc20291284b838666c9e2ed53e953",
      "66c9130786a24d819aaf8babdb7898f7",
      "0b9f1890bee34c1881b046984c7200e5",
      "83ed91d4e4df4105823583f6e79bfb82",
      "6707e5902a8b4a9fb9e1ee1080868c82",
      "26aeb74635b84b0fabba978829364c04",
      "bb07731268484c7eaf55233785851cdc",
      "9f4dfa384d2747c0800382adf608e841",
      "687f8f293d2847f48b152951f339cbbc",
      "659aa74a792749d4b9a2be519d75cb6b",
      "4bd83f3195c0460dbf2c61e06ee902bc",
      "c1a7d68559c6452cba497a8e73266713",
      "d15af26a50494927a6bce930a308ecd5",
      "40148a97df7743e3aad0d671704d1e73",
      "4c7afa578bdf4662858e4a9b362f344b",
      "3bad11c0c9fc4c478dd6fc7b4af761a8",
      "35adf0f713354fcd8cf321b8279b1020",
      "f415e729b84b4571b5812fc9b8ffb978",
      "970af3df90f14efeaf742b714d96cc4f",
      "463c5d5cf3c3460b9971fcfb20349263",
      "85f3483456734d01be54a618953e6e91",
      "2f91b5ac498f44cfb1b797247ce0796c",
      "fac3987fc3ce49aaa03bad8b694240cc",
      "d3908b6cabc34276940ec4918c886f3a",
      "3a2185ec3f2c4c589adbba161c420701",
      "8c6442f769e3444cb14088d11b5ed9c7",
      "fd1e23a5aed4435c85ac8fcb7d217ea5",
      "3108c95a6ca94f03945e61c694be5573",
      "7be4d1b325934bad9b0f6f8e38fd1706",
      "55570d6e39be49b18febfbeb2d0c6877",
      "a883d7dfe17f4e20ada1274014fa06e7",
      "dd8f891363ad47ad83c2419597cf924d",
      "68fe2dba44154146af600b89ef277bb6",
      "e30f942ae7114306b0dbfc46832d6c50",
      "429f22a6c39141a68c58393a454c86fd",
      "a10bc024306e49bf9c9ee3017122dc86",
      "1d56d3a9413b4be9848bdd024e42494b",
      "3494824fa088488a80e77046bf796b00"
     ]
    },
    "outputId": "b1fef7e9-867a-4305-d15a-be07e6fa7ebc"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9f043557e0f4a84a47891ad77170528"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32130cefb1094fd4b453d7137882244b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be7662a7eaee4f4080e258f287640db3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b7183245ef043ca85a66e7c397722b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26aeb74635b84b0fabba978829364c04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35adf0f713354fcd8cf321b8279b1020"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3108c95a6ca94f03945e61c694be5573"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cs_model.eval()\n",
    "cs_outputs = []\n",
    "batch_size = 4  # increase if GPU allows\n",
    "total = len(generation_inputs)\n",
    "\n",
    "# Cache for speed\n",
    "tokenizer = cs_tokenizer\n",
    "apply_template = tokenizer.apply_chat_template\n",
    "device = cs_model.device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = generation_inputs[start:start + batch_size]\n",
    "\n",
    "        # Apply chat template\n",
    "        prompts = [\n",
    "            apply_template(\n",
    "                msg,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for msg in batch[\"messages\"]\n",
    "        ]\n",
    "\n",
    "        # Tokenize with padding\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate (FAST PATH)\n",
    "        outputs = cs_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,  # GREEDY = FAST\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "        input_lens = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Decode outputs\n",
    "        for i, output in enumerate(outputs):\n",
    "            response = tokenizer.decode(\n",
    "                output[input_lens:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            cs_outputs.append((batch[\"messages\"][i], response))\n",
    "\n",
    "        progress = min(start + batch_size, total) / total * 100\n",
    "        print(f\"{progress:.2f}% complete\")\n",
    ""
   ],
   "metadata": {
    "id": "n-P0H8Ka6fBD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "35d6de6a-a507-4174-a79c-ef0710ea04a3"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.96% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.92% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15.84% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19.80% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "27.72% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31.68% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39.60% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "43.56% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "51.49% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55.45% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "63.37% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "67.33% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "75.25% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "79.21% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87.13% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "91.09% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "95.05% complete\n",
      "99.01% complete\n",
      "100.00% complete\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Path(\"cs_outptus.json\").write_text(json.dumps(cs_outputs))"
   ],
   "metadata": {
    "id": "bNoTmVkr8ON1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "91f6a494-a883-467e-9d33-9e74f61169e6"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2139630"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "del cs_model\n",
    "del cs_tokenizer\n",
    "collect()"
   ],
   "metadata": {
    "id": "Ryigh6f18Ven",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f34d8b3-71f2-429d-bb89-05b684a851ed"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12296"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_3 Results\n",
    "\n",
    "Results from the Model which was Fine Tuned with the TL;DR and then Custom Dataset"
   ],
   "metadata": {
    "id": "OZDsur0QElpa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ADAPTER_PATH = \"./final-summary\""
   ],
   "metadata": {
    "id": "9cRueZwwEWvV"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cts_tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "cts_tokenizer.pad_token = cts_tokenizer.eos_token\n",
    "cts_tokenizer.padding_side = \"right\"\n",
    "cts_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,  # SAME as original\n",
    ")\n",
    "\n",
    "cts_model = prepare_model_for_kbit_training(cts_model, use_gradient_checkpointing=True)\n",
    "cts_model = PeftModel.from_pretrained(\n",
    "    cts_model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=True   # to continue training\n",
    ")\n"
   ],
   "metadata": {
    "id": "Gq92Z0zM8zwT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "184427a87b174d7f84b3037006dff25f",
      "22683058ffe140de8036df070d32601a",
      "2edad67e331f4696bbac92ade3074114",
      "eb86c8ad097645e195c75919c95eb9fa",
      "07e6ef980f0043ada178117278ca1286",
      "460ca67a400e4b9c88b26e87db5f2162",
      "bda4f3e8dc054d90852c842a71911319",
      "12f4679debf34eca8a6e60f20d51f480",
      "006687755b004206a7caf0e537176740",
      "e35f70f3f4814e3c88d307f46607d925",
      "5fb947ae3aca4a6b94ae6777c66ee055"
     ]
    },
    "outputId": "dfe44b88-0baa-4890-c7ba-71f4795a9f5e"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "184427a87b174d7f84b3037006dff25f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cts_model.eval()\n",
    "cts_outputs = []\n",
    "batch_size = 6  # increase if GPU allows\n",
    "total = len(generation_inputs)\n",
    "\n",
    "# Cache for speed\n",
    "tokenizer = cts_tokenizer\n",
    "apply_template = tokenizer.apply_chat_template\n",
    "device = cts_model.device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = generation_inputs[start:start + batch_size]\n",
    "\n",
    "        # Apply chat template\n",
    "        prompts = [\n",
    "            apply_template(\n",
    "                msg,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for msg in batch[\"messages\"]\n",
    "        ]\n",
    "\n",
    "        # Tokenize with padding\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate (FAST PATH)\n",
    "        outputs = cts_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,  # GREEDY = FAST\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "        input_lens = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        # Decode outputs\n",
    "        for i, output in enumerate(outputs):\n",
    "            response = tokenizer.decode(\n",
    "                output[input_lens:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            cts_outputs.append((batch[\"messages\"][i], response))\n",
    "\n",
    "        progress = min(start + batch_size, total) / total * 100\n",
    "        print(f\"{progress:.2f}% complete\")\n",
    "\n",
    "\n",
    ""
   ],
   "metadata": {
    "id": "6-NmKzGsEzdH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31548a6d-5c86-493f-bf56-fa111da2fbaf"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.94% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17.82% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29.70% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "41.58% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "53.47% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65.35% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "77.23% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "89.11% complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "95.05% complete\n",
      "100.00% complete\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Path(\"cts_outputs.json\").write_text(json.dumps(cts_outputs))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99d23a1a-e55c-446a-883b-5ec30fcf906a",
    "id": "d3wrUvRVMVYA"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2208586"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ]
  }
 ]
}
