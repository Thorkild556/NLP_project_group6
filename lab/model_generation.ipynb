{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "\n",
    "AIM of this Notebook is to collect the outputs from \n",
    "1. base_model (LLAMA 3B)\n",
    "2. Model_1 (LLAMA 3B Model fine-tuned with TL;DR and Custom Dataset)\n",
    "3. Model_2 (LLAMA 3B Model Fine-tuned with Custom Dataset)\n",
    "\n",
    "which would be later be used for its [evalution](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/model_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nlG_0Y4mJZL",
    "outputId": "00bd43bb-0990-4ec7-f12f-f3ad21b4a577",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip fine_tuned_with_cs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz6x3ul0m5-F",
    "outputId": "030f88f4-3caa-4e1a-b57b-bc5315288363"
   },
   "outputs": [],
   "source": [
    "!unzip final-summary.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e3k7193nCKs",
    "outputId": "5dcc04cc-d96f-4bb3-fda3-6030f903c91c"
   },
   "outputs": [],
   "source": [
    "!pip install pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBUjeex7nC-l",
    "outputId": "197d658d-7198-4728-c099-df757be60b41"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8doJAbBgnEqG",
    "outputId": "f647b768-9f64-486f-f9d2-1e64cdf66f8a"
   },
   "outputs": [],
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # or Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "# we are doing this so we can import src folder\n",
    "\n",
    "import json\n",
    "from gc import collect\n",
    "from src.utils.torch import ensure_device\n",
    "from src.load_dataset import load_jsonl, CS_JSON, split_90_and_10\n",
    "from src.load_model import load_tokenizer, load_model, lora_config_for\n",
    "from src.extract_from import msg_for_base_model, non_assistant_messages\n",
    "from src.train_model import EXPORT_CS_FINE_TUNED, EXPORT_TLDR_CS_FINE_TUNED\n",
    "from src.eval_model import linearly_infer_from, batch_infer_from, EXPORT_CS_RESULTS, EXPORT_BASE_RESULTS, \\\n",
    "    EXPORT_CS_TLDR_RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRrDs_f6na6D",
    "outputId": "dfc7a953-a647-4b06-a46c-9dc2006f826a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "source": [
    "ensure_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeIfjgbnnd1R",
    "outputId": "c5984b28-81c0-4f46-824a-ae0012daaffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "✓ Loaded 101 examples\n"
     ]
    }
   ],
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "\n",
    "custom_dataset = load_jsonl(CS_JSON)\n",
    "val_dataset = split_90_and_10(custom_dataset)[\"test\"]\n",
    "print(f\"✓ Loaded {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Samples\n",
    "\n",
    "Every Sample in the JSONL has three messages (system instruction, user message and then the assistant response)\n",
    "and since we wanted to collect the assistant responses from models we have, we would only extract system instruction and user message from custom dataset.\n",
    "\n",
    "we make sure to make the instructions clear for the base model as it was not fine-tuned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e246372933c94db8a63bfb7117549a02",
      "83626bd183674f1abb0bb2e0a271eec4",
      "77ae32ce5a2f47e0bdf800d04af5dbc4",
      "01511b6c2ef44f4f9415dd13e7fba65d",
      "ac340b8566614c93b7209e06f427135d",
      "d2f357bb5f60476da2c1da71fb18117d",
      "7f60902bd40948c99d2ca74e770c562b",
      "bd319f8283f94806abe665391dc2eeef",
      "0f17a701f1fb4ca68dfbb95c18bff948",
      "b253649bc1fb44119f1a65dcb0ded250",
      "9463590d7be34d4fbb9f511035f194af"
     ]
    },
    "id": "luCjvuLNnuan",
    "outputId": "02b297f4-ecb0-4230-bf2f-49147a110a70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e246372933c94db8a63bfb7117549a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_generation_inputs = val_dataset.map(msg_for_base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "we have log-in inside hugging face so we can access [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) as it's a gated repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o22n7Z60oIa6",
    "outputId": "66a101ed-3884-44e3-b5ca-4098d5850d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001B[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtV48h77BYuB"
   },
   "source": [
    "# Model_0 Results\n",
    "\n",
    "Results from the Base Model (LLMA 3.2 3B Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607,
     "referenced_widgets": [
      "ca7a197568a844f2adf04b6b25f6b534",
      "4134cf11b062487cbb0df5edd6db1db3",
      "18e130a630c74b3f93e27f839b2d60b0",
      "85caca4fed2c4ef3a8ebc65e636aaa48",
      "96d0212dbd904de5b4ea538ebf77cb88",
      "b8050793ad9248b0acb3e41c4e4197d6",
      "1bda686a167a4e7bb8d100fe6d3db42c",
      "33c739194f554f24b993a56d222143d5",
      "3de04e2042de4ce09f87951e4a6fabf0",
      "68c5262aefe744fc9024aa605482e329",
      "c84f9f33439c42b89ab945934c14bf3b",
      "9b4b490954ab43b3b70cb01c194a6b0d",
      "873ff0c9d5794779bb5ac454459219cc",
      "7223c811502841a8953c986e50a1e468",
      "90988ea8ac93476c8124f78de38ae7a5",
      "37ea064da30a4cbd8148d559e24f3769",
      "4b9884ddf04e441d90db00b8ca2726b8",
      "2c171fe7ed4e47af8b68b24dfc28c582",
      "f38fa0bdb4284c25abfd6c8bbeda4226",
      "b1287d91d7034345b9f5f07e7b2ee0e0",
      "4fb854600d734388a9c112244f9ed68b",
      "eab50cb542ee443380efb9f64bd2d3ef",
      "33fc78b5f4da4c189531078ce4d08203",
      "7b4e1d1eeaac486290ee99cbd50e74c2",
      "52748c86132a41cca782f7c0ac419b13",
      "3b57e3b760cc42ec97bade8924372760",
      "257fe6e46f9b49649779b9799e65354f",
      "806e867443c34473a4c53368fae50862",
      "e664cbbee2b5499583c7c58e1ac3376c",
      "e2c84fcd35294aef86ab2a599bd19d4a",
      "359a1261a9cc48c68defa46d703f0d58",
      "4234f387ccb540d194e9e4c21e35d846",
      "0e75fb3486834667b9cba8071c971a17"
     ]
    },
    "id": "Gix46p4fn827",
    "outputId": "ae2a9629-b111-47ea-916a-c49f987ce4a3"
   },
   "outputs": [],
   "source": [
    "base_tokenizer = load_tokenizer()\n",
    "base_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAfePBtFqK_c",
    "outputId": "aa6a2985-9aea-4096-ead9-ffb041a399da"
   },
   "outputs": [],
   "source": [
    "base_outputs = linearly_infer_from(base_model, base_tokenizer, base_generation_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "\n",
    "we would now export the results list of (prompt_message, assistant response) to json file\n",
    "\n",
    "we have observed with T4 GPU it took ~1.5 hours just for inference we would be trying out with higher GPU and inferencing in batches in next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(EXPORT_BASE_RESULTS).write_text(json.dumps(base_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del base_model\n",
    "del base_tokenizer\n",
    "collect()\n",
    "\n",
    "# we are doing this to make sure the python's garbage collector collects previous model and tokenizer to save gpu ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuDDewizBhtg"
   },
   "source": [
    "# Model_1 Results\n",
    "\n",
    "Results from the Model (LLMA 3.2 3B Instruct) which was Fine-tuned with only the Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "371c50340cf4441e9f489790f906e76e",
      "49aed83ef0d34d88bc8d1bf90ddd2df0",
      "6e7bcb603eba4840bef1355462da7f21",
      "82f5c1f4893d4d758fa22824aa43bf6e",
      "d8e47bcc42f341a882c03240d2c93d76",
      "dd9fc1f01515430391e19df9b58612c6",
      "6979a74373fc4967a94c70ce351e04e3",
      "5f2a0a376b414764aed2267ad360060e",
      "7b91558969f04a25928078b96353892a",
      "612fe551a77e4c7caffafe81089324f2",
      "337dac6b92114f99b8b70aa4996932a5"
     ]
    },
    "id": "uEjUfcyr6nd3",
    "outputId": "2a5c0e28-407d-469d-ba31-f9f1374a71da"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371c50340cf4441e9f489790f906e76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation_inputs = val_dataset.map(non_assistant_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "f9f043557e0f4a84a47891ad77170528",
      "051c6c7e1ccb4fe88b83e092ab4fbe21",
      "852995dc0a064e88afed862598c574fc",
      "a82b867afca4466e8aa939ff9f783e9b",
      "3c9d85a77e2846a7a7da95e97eb7cbde",
      "fea9fb84b553492bbb39e73bf80d03f1",
      "833183f39ed5445381f5b89297d7d52c",
      "524cabdab23d424ebff70f9c5cd5491e",
      "6d1f0517e85d4c83ac10762dbafaec4a",
      "592432bea4814f5595a12c10dcc80d19",
      "65256af0db6e413d9c7362af34541166",
      "32130cefb1094fd4b453d7137882244b",
      "38e827ca6bd3401f811c23dc16bc9a1e",
      "6cf033cbd057462da489c189cc20a6dc",
      "7a493593a4f84e0a84587f2df117301c",
      "ad7b8df1d8df4bcf9eba4f4a70a77449",
      "ba2378c746894a4783667b939e07f84b",
      "a10d4028ea7f42bc803b465c6289fc5e",
      "716e56e32ce24b2186e71345c4072f22",
      "df12d61aab5b4df9959eb3d33b2242d0",
      "fad5a8bffb5c4961a4cd1c6534756351",
      "8fd9260c0d2c43c2ba93019179d75a08",
      "be7662a7eaee4f4080e258f287640db3",
      "f0d7aa4dbf734425b2cd191a9e48b225",
      "0a813b46adb44bec9b6b88e90e736ca5",
      "e5cacb0b49384e308d524b9eb1a51c70",
      "b3f52afc1a314a9e8d12a51c1d17f143",
      "e079f7d49754476e86ca670099d18bc5",
      "b315027d5d83445a85fca7712d80f0df",
      "991995c8e7184783b660df270062ff52",
      "fe65fb9e3d9a41039b5055fb5335c5d3",
      "e457cdfb297149779401a6eed7a46c9a",
      "6874813e81fc49c0abf6ca926903667c",
      "0b7183245ef043ca85a66e7c397722b1",
      "46f916ee0be64de3b948d99d067e7a00",
      "c80ca335dec74768b5722cb629380740",
      "d70e5f2d02954f00aff05b0242a44b1c",
      "d35b3cecf18c4950bc362372b5c40564",
      "09ee8ae67fde4916bd65a925c54a0eb9",
      "834bc20291284b838666c9e2ed53e953",
      "66c9130786a24d819aaf8babdb7898f7",
      "0b9f1890bee34c1881b046984c7200e5",
      "83ed91d4e4df4105823583f6e79bfb82",
      "6707e5902a8b4a9fb9e1ee1080868c82",
      "26aeb74635b84b0fabba978829364c04",
      "bb07731268484c7eaf55233785851cdc",
      "9f4dfa384d2747c0800382adf608e841",
      "687f8f293d2847f48b152951f339cbbc",
      "659aa74a792749d4b9a2be519d75cb6b",
      "4bd83f3195c0460dbf2c61e06ee902bc",
      "c1a7d68559c6452cba497a8e73266713",
      "d15af26a50494927a6bce930a308ecd5",
      "40148a97df7743e3aad0d671704d1e73",
      "4c7afa578bdf4662858e4a9b362f344b",
      "3bad11c0c9fc4c478dd6fc7b4af761a8",
      "35adf0f713354fcd8cf321b8279b1020",
      "f415e729b84b4571b5812fc9b8ffb978",
      "970af3df90f14efeaf742b714d96cc4f",
      "463c5d5cf3c3460b9971fcfb20349263",
      "85f3483456734d01be54a618953e6e91",
      "2f91b5ac498f44cfb1b797247ce0796c",
      "fac3987fc3ce49aaa03bad8b694240cc",
      "d3908b6cabc34276940ec4918c886f3a",
      "3a2185ec3f2c4c589adbba161c420701",
      "8c6442f769e3444cb14088d11b5ed9c7",
      "fd1e23a5aed4435c85ac8fcb7d217ea5",
      "3108c95a6ca94f03945e61c694be5573",
      "7be4d1b325934bad9b0f6f8e38fd1706",
      "55570d6e39be49b18febfbeb2d0c6877",
      "a883d7dfe17f4e20ada1274014fa06e7",
      "dd8f891363ad47ad83c2419597cf924d",
      "68fe2dba44154146af600b89ef277bb6",
      "e30f942ae7114306b0dbfc46832d6c50",
      "429f22a6c39141a68c58393a454c86fd",
      "a10bc024306e49bf9c9ee3017122dc86",
      "1d56d3a9413b4be9848bdd024e42494b",
      "3494824fa088488a80e77046bf796b00"
     ]
    },
    "id": "lsD3ZcnutQXs",
    "outputId": "b1fef7e9-867a-4305-d15a-be07e6fa7ebc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f043557e0f4a84a47891ad77170528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32130cefb1094fd4b453d7137882244b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7662a7eaee4f4080e258f287640db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7183245ef043ca85a66e7c397722b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26aeb74635b84b0fabba978829364c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35adf0f713354fcd8cf321b8279b1020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3108c95a6ca94f03945e61c694be5573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_tokenizer = load_tokenizer(EXPORT_CS_FINE_TUNED)\n",
    "\n",
    "cs_model = load_model(EXPORT_CS_FINE_TUNED)\n",
    "cs_model = lora_config_for(cs_model, EXPORT_CS_FINE_TUNED, for_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "from the previous run we have observed th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-P0H8Ka6fBD",
    "outputId": "35d6de6a-a507-4174-a79c-ef0710ea04a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.96% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.92% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.84% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.80% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.72% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.68% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.60% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.56% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.49% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.37% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.33% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.25% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.21% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.13% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.09% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.05% complete\n",
      "99.01% complete\n",
      "100.00% complete\n"
     ]
    }
   ],
   "source": [
    "cs_outputs = batch_infer_from(cs_model, cs_tokenizer, generation_inputs, batch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "\n",
    "we would now export the results list of (prompt_message, assistant response) to json file\n",
    "\n",
    "we have observed with A100 GPU it took ~1 hour for inference since we have tried to utilize more GPU (with batch: 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(EXPORT_CS_RESULTS).write_text(json.dumps(cs_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ryigh6f18Ven",
    "outputId": "8f34d8b3-71f2-429d-bb89-05b684a851ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12296"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cs_model\n",
    "del cs_tokenizer\n",
    "collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_2 Results\n",
    "\n",
    "Results from the Model which was Fine-Tuned with the TL;DR and then Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "184427a87b174d7f84b3037006dff25f",
      "22683058ffe140de8036df070d32601a",
      "2edad67e331f4696bbac92ade3074114",
      "eb86c8ad097645e195c75919c95eb9fa",
      "07e6ef980f0043ada178117278ca1286",
      "460ca67a400e4b9c88b26e87db5f2162",
      "bda4f3e8dc054d90852c842a71911319",
      "12f4679debf34eca8a6e60f20d51f480",
      "006687755b004206a7caf0e537176740",
      "e35f70f3f4814e3c88d307f46607d925",
      "5fb947ae3aca4a6b94ae6777c66ee055"
     ]
    },
    "id": "Gq92Z0zM8zwT",
    "outputId": "dfe44b88-0baa-4890-c7ba-71f4795a9f5e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184427a87b174d7f84b3037006dff25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cts_tokenizer = load_tokenizer(EXPORT_TLDR_CS_FINE_TUNED)\n",
    "\n",
    "cts_model = load_model(EXPORT_TLDR_CS_FINE_TUNED)\n",
    "cts_model = lora_config_for(cts_model, EXPORT_TLDR_CS_FINE_TUNED, for_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-NmKzGsEzdH",
    "outputId": "31548a6d-5c86-493f-bf56-fa111da2fbaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.94% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.82% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.70% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.58% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.47% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.35% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.23% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.11% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.05% complete\n",
      "100.00% complete\n"
     ]
    }
   ],
   "source": [
    "cts_outputs = batch_infer_from(cts_model, cts_tokenizer, generation_inputs, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3wrUvRVMVYA",
    "outputId": "99d23a1a-e55c-446a-883b-5ec30fcf906a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208586"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(EXPORT_CS_TLDR_RESULTS).write_text(json.dumps(cts_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "we have exported the results from the Model_2 as well. please refer to this notebook: [model_evaluation](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/model_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
