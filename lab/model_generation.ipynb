{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AIM\n",
    "\n",
    "AIM of this Notebook is to collect the outputs from base_model (LLAMA 3B), Model_1 (LLAMA 3B Model fine-tuned with TL;DR and Custom Dataset) and Model_2 (LLAMA 3B Model Fine-tuned with Custom Dataset) which would be later used for its evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nlG_0Y4mJZL",
    "outputId": "00bd43bb-0990-4ec7-f12f-f3ad21b4a577"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  fine_tuned_with_cs.zip\n",
      "   creating: llama3.2-3b-qlora-summary/\n",
      "  inflating: llama3.2-3b-qlora-summary/README.md  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-100/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-100/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/training_args.bin  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-171/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-171/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip fine_tuned_with_cs.zip"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!unzip final-summary.zip"
   ],
   "metadata": {
    "id": "Zz6x3ul0m5-F",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "030f88f4-3caa-4e1a-b57b-bc5315288363"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  final-summary.zip\n",
      "   creating: final-summary/\n",
      "  inflating: final-summary/README.md  \n",
      "   creating: final-summary/checkpoint-100/\n",
      "  inflating: final-summary/checkpoint-100/README.md  \n",
      "  inflating: final-summary/checkpoint-100/adapter_config.json  \n",
      "  inflating: final-summary/checkpoint-100/training_args.bin  \n",
      "  inflating: final-summary/checkpoint-100/special_tokens_map.json  \n",
      "  inflating: final-summary/checkpoint-100/tokenizer_config.json  \n",
      "  inflating: final-summary/checkpoint-100/rng_state.pth  \n",
      "  inflating: final-summary/checkpoint-100/trainer_state.json  \n",
      "  inflating: final-summary/checkpoint-100/chat_template.jinja  \n",
      "  inflating: final-summary/checkpoint-100/optimizer.pt  \n",
      "  inflating: final-summary/checkpoint-100/scheduler.pt  \n",
      "  inflating: final-summary/checkpoint-100/adapter_model.safetensors  \n",
      "  inflating: final-summary/checkpoint-100/tokenizer.json  \n",
      "  inflating: final-summary/adapter_config.json  \n",
      "  inflating: final-summary/training_args.bin  \n",
      "   creating: final-summary/checkpoint-171/\n",
      "  inflating: final-summary/checkpoint-171/README.md  \n",
      "  inflating: final-summary/checkpoint-171/adapter_config.json  \n",
      "  inflating: final-summary/checkpoint-171/training_args.bin  \n",
      "  inflating: final-summary/checkpoint-171/special_tokens_map.json  \n",
      "  inflating: final-summary/checkpoint-171/tokenizer_config.json  \n",
      "  inflating: final-summary/checkpoint-171/rng_state.pth  \n",
      "  inflating: final-summary/checkpoint-171/trainer_state.json  \n",
      "  inflating: final-summary/checkpoint-171/chat_template.jinja  \n",
      "  inflating: final-summary/checkpoint-171/optimizer.pt  \n",
      "  inflating: final-summary/checkpoint-171/scheduler.pt  \n",
      "  inflating: final-summary/checkpoint-171/adapter_model.safetensors  \n",
      "  inflating: final-summary/checkpoint-171/tokenizer.json  \n",
      "  inflating: final-summary/special_tokens_map.json  \n",
      "  inflating: final-summary/tokenizer_config.json  \n",
      "  inflating: final-summary/chat_template.jinja  \n",
      "  inflating: final-summary/adapter_model.safetensors  \n",
      "  inflating: final-summary/tokenizer.json  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e3k7193nCKs",
    "outputId": "5dcc04cc-d96f-4bb3-fda3-6030f903c91c"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers torch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBUjeex7nC-l",
    "outputId": "197d658d-7198-4728-c099-df757be60b41"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8doJAbBgnEqG",
    "outputId": "f647b768-9f64-486f-f9d2-1e64cdf66f8a"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Collecting torch==2.9.1 (from xformers)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->xformers)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.9/122.9 MB\u001B[0m \u001B[31m18.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m899.7/899.7 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m594.3/594.3 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m139.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.0/88.0 MB\u001B[0m \u001B[31m25.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m954.8/954.8 kB\u001B[0m \u001B[31m63.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.1/193.1 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m72.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.6/63.6 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m267.5/267.5 MB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/c2/f5/e1854cb2f2bcd4280c44736c93550cc300ff4b8c95ebe370d0aa7d2b473d/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001B[0m\u001B[33m\n",
      "\u001B[0mDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m288.2/288.2 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.3/39.3 MB\u001B[0m \u001B[31m54.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m170.5/170.5 MB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m518.9/518.9 kB\u001B[0m \u001B[31m43.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.1/59.1 MB\u001B[0m \u001B[31m39.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, bitsandbytes, trl\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1 trl-0.26.2 xformers-0.0.33.post2\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent  # or Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "# we are doing this so we can import src folder\n",
    "\n",
    "import json\n",
    "from gc import collect\n",
    "from src.utils.torch import ensure_device\n",
    "from src.load_dataset import load_jsonl, CS_JSON, split\n",
    "from src.load_model import load_tokenizer, load_model, lora_config_for\n",
    "from src.extract_from import msg_for_base_model, non_assistant_messages\n",
    "from src.train_model import EXPORT_CS_FINE_TUNED, EXPORT_TLDR_CS_FINE_TUNED\n",
    "from src.eval_model import linearly_infer_from, batch_infer_from, EXPORT_CS_RESULTS, EXPORT_BASE_RESULTS, \\\n",
    "    EXPORT_CS_TLDR_RESULTS"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRrDs_f6na6D",
    "outputId": "dfc7a953-a647-4b06-a46c-9dc2006f826a"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would be using this device: cuda\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": "ensure_device()"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeIfjgbnnd1R",
    "outputId": "c5984b28-81c0-4f46-824a-ae0012daaffc"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "✓ Loaded 101 examples\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "\n",
    "custom_dataset = load_jsonl(CS_JSON)\n",
    "val_dataset = split(custom_dataset)[\"test\"]\n",
    "print(f\"✓ Loaded {len(val_dataset)} examples\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Converting the Samples\n",
    "\n",
    "Every Sample in the JSONL has three messages (system instruction, user message and then the assistant response)\n",
    "and since we wanted to collect the assistant responses from models we have, we would only extract system instruction and user message from custom dataset.\n",
    "\n",
    "we make sure to make the instructions clear for the base model as it was not fine-tuned before."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e246372933c94db8a63bfb7117549a02",
      "83626bd183674f1abb0bb2e0a271eec4",
      "77ae32ce5a2f47e0bdf800d04af5dbc4",
      "01511b6c2ef44f4f9415dd13e7fba65d",
      "ac340b8566614c93b7209e06f427135d",
      "d2f357bb5f60476da2c1da71fb18117d",
      "7f60902bd40948c99d2ca74e770c562b",
      "bd319f8283f94806abe665391dc2eeef",
      "0f17a701f1fb4ca68dfbb95c18bff948",
      "b253649bc1fb44119f1a65dcb0ded250",
      "9463590d7be34d4fbb9f511035f194af"
     ]
    },
    "id": "luCjvuLNnuan",
    "outputId": "02b297f4-ecb0-4230-bf2f-49147a110a70"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e246372933c94db8a63bfb7117549a02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10,
   "source": "base_generation_inputs = val_dataset.map(msg_for_base_model)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Note\n",
    "\n",
    "we have log-in inside hugging face so we can access [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) as it's a gated repo."
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o22n7Z60oIa6",
    "outputId": "66a101ed-3884-44e3-b5ca-4098d5850d50"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001B[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA-DEV` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA-DEV`\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": "!huggingface-cli login"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_0 Results\n",
    "\n",
    "Results from the Base Model (LLMA 3.2 3B Instruct)"
   ],
   "metadata": {
    "id": "mtV48h77BYuB"
   }
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607,
     "referenced_widgets": [
      "ca7a197568a844f2adf04b6b25f6b534",
      "4134cf11b062487cbb0df5edd6db1db3",
      "18e130a630c74b3f93e27f839b2d60b0",
      "85caca4fed2c4ef3a8ebc65e636aaa48",
      "96d0212dbd904de5b4ea538ebf77cb88",
      "b8050793ad9248b0acb3e41c4e4197d6",
      "1bda686a167a4e7bb8d100fe6d3db42c",
      "33c739194f554f24b993a56d222143d5",
      "3de04e2042de4ce09f87951e4a6fabf0",
      "68c5262aefe744fc9024aa605482e329",
      "c84f9f33439c42b89ab945934c14bf3b",
      "9b4b490954ab43b3b70cb01c194a6b0d",
      "873ff0c9d5794779bb5ac454459219cc",
      "7223c811502841a8953c986e50a1e468",
      "90988ea8ac93476c8124f78de38ae7a5",
      "37ea064da30a4cbd8148d559e24f3769",
      "4b9884ddf04e441d90db00b8ca2726b8",
      "2c171fe7ed4e47af8b68b24dfc28c582",
      "f38fa0bdb4284c25abfd6c8bbeda4226",
      "b1287d91d7034345b9f5f07e7b2ee0e0",
      "4fb854600d734388a9c112244f9ed68b",
      "eab50cb542ee443380efb9f64bd2d3ef",
      "33fc78b5f4da4c189531078ce4d08203",
      "7b4e1d1eeaac486290ee99cbd50e74c2",
      "52748c86132a41cca782f7c0ac419b13",
      "3b57e3b760cc42ec97bade8924372760",
      "257fe6e46f9b49649779b9799e65354f",
      "806e867443c34473a4c53368fae50862",
      "e664cbbee2b5499583c7c58e1ac3376c",
      "e2c84fcd35294aef86ab2a599bd19d4a",
      "359a1261a9cc48c68defa46d703f0d58",
      "4234f387ccb540d194e9e4c21e35d846",
      "0e75fb3486834667b9cba8071c971a17"
     ]
    },
    "id": "Gix46p4fn827",
    "outputId": "ae2a9629-b111-47ea-916a-c49f987ce4a3"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_tokenizer = load_tokenizer()\n",
    "base_model = load_model()"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oAfePBtFqK_c",
    "outputId": "aa6a2985-9aea-4096-ead9-ffb041a399da"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4472\n",
      "2173\n",
      "2588\n",
      "3493\n",
      "1333\n",
      "7101\n",
      "3200\n",
      "2747\n",
      "3450\n",
      "2684\n",
      "2382\n",
      "5195\n",
      "3245\n",
      "2329\n",
      "3437\n",
      "1777\n",
      "3072\n",
      "1929\n",
      "2397\n",
      "3252\n",
      "2971\n",
      "3297\n",
      "2854\n",
      "2745\n",
      "2402\n",
      "3726\n",
      "1992\n",
      "2768\n",
      "1412\n",
      "3116\n",
      "3023\n",
      "2304\n",
      "5197\n",
      "3501\n",
      "3816\n",
      "2858\n",
      "2933\n",
      "1648\n",
      "2644\n",
      "3583\n",
      "3513\n",
      "2842\n",
      "2248\n",
      "2041\n",
      "2094\n",
      "1816\n",
      "4051\n",
      "1813\n",
      "2608\n",
      "3987\n",
      "2073\n",
      "2224\n",
      "2359\n",
      "2120\n",
      "3774\n",
      "3856\n",
      "3009\n",
      "2167\n",
      "3069\n",
      "2287\n",
      "3755\n",
      "3099\n",
      "2327\n",
      "4028\n",
      "2822\n",
      "2137\n",
      "2387\n",
      "3402\n",
      "1843\n",
      "2294\n",
      "2069\n",
      "2056\n",
      "3564\n",
      "6023\n",
      "2666\n",
      "5656\n",
      "3596\n",
      "2184\n",
      "2093\n",
      "4498\n",
      "2338\n",
      "2207\n",
      "1648\n",
      "5847\n",
      "3027\n",
      "3232\n",
      "5577\n"
     ]
    }
   ],
   "execution_count": null,
   "source": "base_outputs = linearly_infer_from(base_model, base_tokenizer, base_generation_inputs)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exporting\n",
    "\n",
    "we would now export the results list of (prompt_message, assistant response) to json file\n",
    "\n",
    "we have observed with T4 GPU it took ~1.5 hours just for inference we would be trying out with higher GPU and inferencing in batches in next batch."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Path(EXPORT_BASE_RESULTS).write_text(json.dumps(base_outputs))"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "del base_model\n",
    "del base_tokenizer\n",
    "collect()\n",
    "\n",
    "# we are doing this to make sure the python's garbage collector collects previous model and tokenizer to save gpu ram"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_1 Results\n",
    "\n",
    "Results from the Model (LLMA 3.2 3B Instruct) which was Fine-tuned with only the Custom Dataset"
   ],
   "metadata": {
    "id": "AuDDewizBhtg"
   }
  },
  {
   "cell_type": "code",
   "source": "generation_inputs = val_dataset.map(non_assistant_messages)",
   "metadata": {
    "id": "uEjUfcyr6nd3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "371c50340cf4441e9f489790f906e76e",
      "49aed83ef0d34d88bc8d1bf90ddd2df0",
      "6e7bcb603eba4840bef1355462da7f21",
      "82f5c1f4893d4d758fa22824aa43bf6e",
      "d8e47bcc42f341a882c03240d2c93d76",
      "dd9fc1f01515430391e19df9b58612c6",
      "6979a74373fc4967a94c70ce351e04e3",
      "5f2a0a376b414764aed2267ad360060e",
      "7b91558969f04a25928078b96353892a",
      "612fe551a77e4c7caffafe81089324f2",
      "337dac6b92114f99b8b70aa4996932a5"
     ]
    },
    "outputId": "2a5c0e28-407d-469d-ba31-f9f1374a71da"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "371c50340cf4441e9f489790f906e76e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cs_tokenizer = load_tokenizer(EXPORT_CS_FINE_TUNED)\n",
    "\n",
    "cs_model = load_model(EXPORT_CS_FINE_TUNED)\n",
    "cs_model = lora_config_for(cs_model, EXPORT_CS_FINE_TUNED, for_training=False)"
   ],
   "metadata": {
    "id": "lsD3ZcnutQXs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "f9f043557e0f4a84a47891ad77170528",
      "051c6c7e1ccb4fe88b83e092ab4fbe21",
      "852995dc0a064e88afed862598c574fc",
      "a82b867afca4466e8aa939ff9f783e9b",
      "3c9d85a77e2846a7a7da95e97eb7cbde",
      "fea9fb84b553492bbb39e73bf80d03f1",
      "833183f39ed5445381f5b89297d7d52c",
      "524cabdab23d424ebff70f9c5cd5491e",
      "6d1f0517e85d4c83ac10762dbafaec4a",
      "592432bea4814f5595a12c10dcc80d19",
      "65256af0db6e413d9c7362af34541166",
      "32130cefb1094fd4b453d7137882244b",
      "38e827ca6bd3401f811c23dc16bc9a1e",
      "6cf033cbd057462da489c189cc20a6dc",
      "7a493593a4f84e0a84587f2df117301c",
      "ad7b8df1d8df4bcf9eba4f4a70a77449",
      "ba2378c746894a4783667b939e07f84b",
      "a10d4028ea7f42bc803b465c6289fc5e",
      "716e56e32ce24b2186e71345c4072f22",
      "df12d61aab5b4df9959eb3d33b2242d0",
      "fad5a8bffb5c4961a4cd1c6534756351",
      "8fd9260c0d2c43c2ba93019179d75a08",
      "be7662a7eaee4f4080e258f287640db3",
      "f0d7aa4dbf734425b2cd191a9e48b225",
      "0a813b46adb44bec9b6b88e90e736ca5",
      "e5cacb0b49384e308d524b9eb1a51c70",
      "b3f52afc1a314a9e8d12a51c1d17f143",
      "e079f7d49754476e86ca670099d18bc5",
      "b315027d5d83445a85fca7712d80f0df",
      "991995c8e7184783b660df270062ff52",
      "fe65fb9e3d9a41039b5055fb5335c5d3",
      "e457cdfb297149779401a6eed7a46c9a",
      "6874813e81fc49c0abf6ca926903667c",
      "0b7183245ef043ca85a66e7c397722b1",
      "46f916ee0be64de3b948d99d067e7a00",
      "c80ca335dec74768b5722cb629380740",
      "d70e5f2d02954f00aff05b0242a44b1c",
      "d35b3cecf18c4950bc362372b5c40564",
      "09ee8ae67fde4916bd65a925c54a0eb9",
      "834bc20291284b838666c9e2ed53e953",
      "66c9130786a24d819aaf8babdb7898f7",
      "0b9f1890bee34c1881b046984c7200e5",
      "83ed91d4e4df4105823583f6e79bfb82",
      "6707e5902a8b4a9fb9e1ee1080868c82",
      "26aeb74635b84b0fabba978829364c04",
      "bb07731268484c7eaf55233785851cdc",
      "9f4dfa384d2747c0800382adf608e841",
      "687f8f293d2847f48b152951f339cbbc",
      "659aa74a792749d4b9a2be519d75cb6b",
      "4bd83f3195c0460dbf2c61e06ee902bc",
      "c1a7d68559c6452cba497a8e73266713",
      "d15af26a50494927a6bce930a308ecd5",
      "40148a97df7743e3aad0d671704d1e73",
      "4c7afa578bdf4662858e4a9b362f344b",
      "3bad11c0c9fc4c478dd6fc7b4af761a8",
      "35adf0f713354fcd8cf321b8279b1020",
      "f415e729b84b4571b5812fc9b8ffb978",
      "970af3df90f14efeaf742b714d96cc4f",
      "463c5d5cf3c3460b9971fcfb20349263",
      "85f3483456734d01be54a618953e6e91",
      "2f91b5ac498f44cfb1b797247ce0796c",
      "fac3987fc3ce49aaa03bad8b694240cc",
      "d3908b6cabc34276940ec4918c886f3a",
      "3a2185ec3f2c4c589adbba161c420701",
      "8c6442f769e3444cb14088d11b5ed9c7",
      "fd1e23a5aed4435c85ac8fcb7d217ea5",
      "3108c95a6ca94f03945e61c694be5573",
      "7be4d1b325934bad9b0f6f8e38fd1706",
      "55570d6e39be49b18febfbeb2d0c6877",
      "a883d7dfe17f4e20ada1274014fa06e7",
      "dd8f891363ad47ad83c2419597cf924d",
      "68fe2dba44154146af600b89ef277bb6",
      "e30f942ae7114306b0dbfc46832d6c50",
      "429f22a6c39141a68c58393a454c86fd",
      "a10bc024306e49bf9c9ee3017122dc86",
      "1d56d3a9413b4be9848bdd024e42494b",
      "3494824fa088488a80e77046bf796b00"
     ]
    },
    "outputId": "b1fef7e9-867a-4305-d15a-be07e6fa7ebc"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9f043557e0f4a84a47891ad77170528"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32130cefb1094fd4b453d7137882244b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be7662a7eaee4f4080e258f287640db3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b7183245ef043ca85a66e7c397722b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26aeb74635b84b0fabba978829364c04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35adf0f713354fcd8cf321b8279b1020"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3108c95a6ca94f03945e61c694be5573"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Note\n",
    "\n",
    "from the previous run we have observed th"
   ]
  },
  {
   "metadata": {
    "id": "n-P0H8Ka6fBD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "35d6de6a-a507-4174-a79c-ef0710ea04a3"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.96% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.92% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.84% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.80% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.72% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.68% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.60% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.56% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.49% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.45% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.37% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.33% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.25% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.21% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.13% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.09% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.05% complete\n",
      "99.01% complete\n",
      "100.00% complete\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": "cs_outputs = batch_infer_from(cs_model, cs_tokenizer, generation_inputs, batch=4)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exporting\n",
    "\n",
    "we would now export the results list of (prompt_message, assistant response) to json file\n",
    "\n",
    "we have observed with A100 GPU it took ~1 hour for inference since we have tried to utilize more GPU (with batch: 4)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Path(EXPORT_CS_RESULTS).write_text(json.dumps(cs_outputs))"
  },
  {
   "metadata": {
    "id": "Ryigh6f18Ven",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8f34d8b3-71f2-429d-bb89-05b684a851ed"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12296"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21,
   "source": [
    "del cs_model\n",
    "del cs_tokenizer\n",
    "collect()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model_2 Results\n",
    "\n",
    "Results from the Model which was Fine-Tuned with the TL;DR and then Custom Dataset"
   ]
  },
  {
   "metadata": {
    "id": "Gq92Z0zM8zwT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "184427a87b174d7f84b3037006dff25f",
      "22683058ffe140de8036df070d32601a",
      "2edad67e331f4696bbac92ade3074114",
      "eb86c8ad097645e195c75919c95eb9fa",
      "07e6ef980f0043ada178117278ca1286",
      "460ca67a400e4b9c88b26e87db5f2162",
      "bda4f3e8dc054d90852c842a71911319",
      "12f4679debf34eca8a6e60f20d51f480",
      "006687755b004206a7caf0e537176740",
      "e35f70f3f4814e3c88d307f46607d925",
      "5fb947ae3aca4a6b94ae6777c66ee055"
     ]
    },
    "outputId": "dfe44b88-0baa-4890-c7ba-71f4795a9f5e"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "184427a87b174d7f84b3037006dff25f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31,
   "source": [
    "cts_tokenizer = load_tokenizer(EXPORT_TLDR_CS_FINE_TUNED)\n",
    "\n",
    "cts_model = load_model(EXPORT_TLDR_CS_FINE_TUNED)\n",
    "cts_model = lora_config_for(cts_model, EXPORT_TLDR_CS_FINE_TUNED, for_training=False)"
   ]
  },
  {
   "metadata": {
    "id": "6-NmKzGsEzdH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31548a6d-5c86-493f-bf56-fa111da2fbaf"
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.94% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.88% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.82% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.76% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.70% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.64% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.58% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.52% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.47% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.41% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.35% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.29% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.23% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.17% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.11% complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.05% complete\n",
      "100.00% complete\n"
     ]
    }
   ],
   "execution_count": 32,
   "source": "cts_outputs = batch_infer_from(cts_model, cts_tokenizer, generation_inputs, batch=4)"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "99d23a1a-e55c-446a-883b-5ec30fcf906a",
    "id": "d3wrUvRVMVYA"
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208586"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33,
   "source": "Path(EXPORT_CS_TLDR_RESULTS).write_text(json.dumps(cts_outputs))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "we have exported the results from the Model_2 as well. please refer to this notebook: [model_evaluation](https://github.com/au-nlp/project-milestone-p2-group-6/blob/main/lab/model_evaluation.ipynb)"
   ]
  }
 ]
}
