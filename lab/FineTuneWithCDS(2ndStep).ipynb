{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3mxuyTBS55cs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "10a1a1cc-b395-44db-b196-33ac83e6cd4d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  llama_3b_3_2.zip\n",
      "   creating: llama3.2-3b-qlora-summary/\n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/special_tokens_map.json  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-300/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-300/optimizer.pt  \n",
      "   creating: llama3.2-3b-qlora-summary/checkpoint-391/\n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/scheduler.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/tokenizer_config.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/README.md  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/chat_template.jinja  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/special_tokens_map.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/trainer_state.json  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/rng_state.pth  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/training_args.bin  \n",
      "  inflating: llama3.2-3b-qlora-summary/checkpoint-391/optimizer.pt  \n",
      "  inflating: llama3.2-3b-qlora-summary/adapter_model.safetensors  \n",
      "  inflating: llama3.2-3b-qlora-summary/training_args.bin  \n"
     ]
    }
   ],
   "source": [
    "!unzip llama_3b_3_2.zip"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pandas datasets"
   ],
   "metadata": {
    "id": "F5kBKW7563Ag",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "f2a79213-ca26-425c-db72-20f3175d2181"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers torch"
   ],
   "metadata": {
    "id": "qjswUtcf68Pw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "dd996f54-c0cb-45e0-e637-560acd2e82f3"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install xformers trl peft accelerate bitsandbytes"
   ],
   "metadata": {
    "id": "ATQIMBJA6-AX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "72abe42f-2ee0-4933-b566-78e5571255ed"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.26.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
      "Collecting torch==2.9.1 (from xformers)\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (9.10.2.21)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.1->xformers) (3.3.20)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->xformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->xformers)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.1->xformers) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.26.1-py3-none-any.whl (517 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, xformers, bitsandbytes, trl\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1 trl-0.26.1 xformers-0.0.33.post2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "0j4AGGK763ef"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the Dataset"
   ],
   "metadata": {
    "id": "f2rJ1Q_i7UEa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "print(\"\\n2-[1/8] Loading dataset...\")\n",
    "data = []\n",
    "with open(\"custom_dataset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "custom_dataset = Dataset.from_dict({\n",
    "    \"messages\": [item[\"messages\"] for item in data]\n",
    "})\n",
    "\n",
    "print(f\"\u2713 Loaded {len(custom_dataset)} examples\")"
   ],
   "metadata": {
    "id": "FqWtfaR97TdF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a80a6d2-b841-4950-c8cc-ec23318fe52a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "2-[1/8] Loading dataset...\n",
      "\u2713 Loaded 1004 examples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the Train and the Test set\n",
    "\n",
    "Split 0.1 (90% - Train and 10% Test)"
   ],
   "metadata": {
    "id": "o79dBf7H7csI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Split dataset\n",
    "split_dataset = custom_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "print(f\"\u2713 Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ],
   "metadata": {
    "id": "SYuB4DQA7dJU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3192b680-fe1e-4183-a4d3-d3cc02bf11f3"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Train: 903 | Val: 101\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the FineTuned Model\n",
    "\n",
    "Loading the Finetuned (with TL;DR Dataset) LLma 3b Model"
   ],
   "metadata": {
    "id": "ht_24_aQ7FnZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!huggingface-cli login"
   ],
   "metadata": {
    "id": "bkIFjpYo8hg1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31bba52a-5e4e-4404-9002-e0cfd9ecb1c3"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33m\u26a0\ufe0f  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: read).\n",
      "The token `YTA` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `YTA`\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[2/8] Configuring 8-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "metadata": {
    "id": "Wxss7xph7tqS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31ca7ad1-8d92-4d1e-a3ec-aca1eeeca91f"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[2/8] Configuring 8-bit quantization...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ADAPTER_PATH = \"./llama3.2-3b-qlora-summary\""
   ],
   "metadata": {
    "id": "1Yq8n7kd6331"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "id": "JOEi8Kqm7Bxz"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,  # SAME as original\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    ADAPTER_PATH,\n",
    "    is_trainable=True   # to continue training\n",
    ")\n",
    "\n",
    "model.train()\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(\"\u2713 Fine-tuned model loaded\")"
   ],
   "metadata": {
    "id": "Agl1d9Bi7FRu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525,
     "referenced_widgets": [
      "28e288939f2c423db810e768bb665ad7",
      "c45c1fbb84f74d6caff3a351ece215e1",
      "2f19ba1837fe41dfb5f8b7d24bf2ef30",
      "fdd6947a5f3a45a898f86217340383be",
      "60352afbbc8e4ac3b06643abf42e4778",
      "9c5834c801374f0a966ca959736737ab",
      "90fe1352b94e49e9b456e856d938fe05",
      "cb2658ae97c44cdaba54120ff099bb95",
      "b5af82b2981b4b2f95c9d23e0e12281d",
      "1678597f935b4038a3995af6be8ab98f",
      "afa4d7e1f8e84c9994bbfce3e7269b7a",
      "54368211681a46969e5d35b0c486cd54",
      "c92ec1a039d74c2c8c3d420d93ea29ad",
      "c3afc0a2a36842c39324126a5a47be7a",
      "f83d710e94e9473486633b182ef42f9d",
      "929cf5fcc4c54dc9880645b02aa49019",
      "6ab75f327dd442b385a8a74fadeac732",
      "a7a3c1bca59441b59e11fc527891def5",
      "40ce3231f410402d98ff8de52bbb080c",
      "8f740f89b31a4970989b87d0136e0c10",
      "b90e0551299041cbb10e94c5e39997ec",
      "9b9b15815add4833ba4d3f9c9a0a220d",
      "c78e183297ad4aaaa2068d77585da771",
      "4b5b2a0ebf5548f7a1216cb7528e9eed",
      "e4e823f7e0534449a3098b190997b777",
      "9ee2d2b9210e43d488db5d3a521de9ab",
      "2795341f93b84596a072fff4ba5dd36f",
      "deac29310ee44debbe9950af260223dc",
      "9eab1f9c9920434d98559dc7898f521d",
      "8d9f007b65d54d82b9232f6d7ffcc650",
      "7556d705f38a4aa6803c6920030a49b8",
      "7c5b462d85bf4b918f73c550f45e4488",
      "2c91da5ae3c44187b92d10a7d5415b76",
      "41b20cefdff847339d917930a84fb599",
      "d4168edac6a647b5a98c9d59e13d0a34",
      "65a08533dc264aefa13c14d87f3d0021",
      "6aee3cd03fa140b59439e5a202b0631b",
      "db481d47ccd64bfdadedd4f6d8934234",
      "1552503fb79e4728aabe410df4a44c34",
      "0c6c497588ca414c91418521206c1960",
      "754d5aaee3ec4b31b9b8b3f4596bd4fb",
      "2ee44906562a45a1a8da0ec541d510ec",
      "3b73bc9a112a46faa748349707d7fb81",
      "ca170cacc7114ea6b73fe6a657a91347",
      "bd542cdf04d748419ceb5a80c47fdea2",
      "e3198cceed7c4ffe8c05d2b48fc28cf8",
      "d1f9d93db10b49a68ac8a1d5da58f1c0",
      "39d1a60509454418a00dc271eeec0026",
      "d53fcedfa5ff456c8c5a2b8ad4489437",
      "6312647e40834bfdb447e78fa3e0e3d7",
      "308f1d53f1f2410083ad3ca3e29ae0e8",
      "e03f946d19e74834996599b96f7b28e9",
      "904de34c41d04ae396df0b30a63ab56a",
      "ee6e1a2ca1064882a6f8de720e95d235",
      "4c8466da76624767b8e37db0ada2fc56",
      "afe9fff2f13f42cb82c28b6509ed4910",
      "eadb5bc00fb04fed9bee3353786b17f6",
      "f097e8e8d5be4df59e13decc236e655e",
      "06e17020fd5f43a7be595da8b772c128",
      "a01cdc023e67498eab0142d726b4a2b1",
      "febdc95b9b794659864c3536d9fff8f0",
      "badab8a0c1624b5eabcd0883e4983490",
      "3eca85fdb0344b829ced3e358b884f07",
      "a88cc77950684ac18fd87228b230f5b4",
      "2c61ee56737349c3a460261a3ed996e4",
      "41e6e20b60b941158d791dce1ae4d66f",
      "29943391e97c4ceda2303b3a3ff5e10e",
      "3c2aeff0e7dd4b09b90ccf8e14b35d82",
      "e72dcf11b45e4597a19acae2a018ecb9",
      "8fdff50a4500452d8fdd312d6d0a6cac",
      "9cd46db95f8d467fb99ce035d0718a2a",
      "3b22e67d1b5f42f586bc566deefa864f",
      "49529c3b03184c59bbcf68d0f9ecbf59",
      "a1a5a2aaaa19430a8a23a8e56d685c6c",
      "42214c1e3b1b474882de330e51a373f3",
      "25d22fead85540c985146f9fa470d48c",
      "e4b998eebc8e4066b95e874faebf74ec"
     ]
    },
    "outputId": "6f87266d-83a3-47ef-81ae-22681d36535a"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28e288939f2c423db810e768bb665ad7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54368211681a46969e5d35b0c486cd54"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c78e183297ad4aaaa2068d77585da771"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41b20cefdff847339d917930a84fb599"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd542cdf04d748419ceb5a80c47fdea2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afe9fff2f13f42cb82c28b6509ed4910"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29943391e97c4ceda2303b3a3ff5e10e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Fine-tuned model loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ],
   "metadata": {
    "id": "x0LnD0tW8dvC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "406b787e-b21e-42d1-e6c6-43b9944ed151"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511\n",
      "\n",
      "[5/8] Preparing tokenization...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.peft_config  # for verifying lora config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZcXKahaNVOT",
    "outputId": "d19476bf-dc23-400d-9011-fefb773b4210"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path='meta-llama/Llama-3.2-3B-Instruct', revision=None, inference_mode=False, r=16, target_modules={'q_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def format_and_tokenize(example, force=4096):\n",
    "    # Apply LLaMA chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize (NO padding, NO manual labels)\n",
    "    tokenized = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=force,   # IMPORTANT: increase for long transcripts\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ],
   "metadata": {
    "id": "yDNnjkHY8qe3"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 10_000 for the custom dataset"
   ],
   "metadata": {
    "id": "vx1qqkZr80nU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "format_and_tokenize_for_cds = lambda x: format_and_tokenize(x, force=10_000)"
   ],
   "metadata": {
    "id": "7d7pDT6h80aU"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply tokenization\n",
    "print(\"\u2713 Tokenizing train dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "roVseVT688BG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "53d8d239999e469cb6850103e801c864",
      "0b83dbdc6a19463ea90fcc42eea54b11",
      "decfc913b3004159aa075e07fa583246",
      "9a3d7e7c7d3b40acbe27918bf82dffb4",
      "3801984aae9a44679d69089ad521b643",
      "e1677499cfe94deea12371b6aae59fa4",
      "cc7733e013164e7d933217651afd789a",
      "7bde5cc3262c45d282dcba39e2259c2f",
      "c9e2a5ed59d84ec396b50b793e57d461",
      "dfe50d9480174ab3947ce61f30ad6d3e",
      "8877d3829953482b8301d61674e9a623"
     ]
    },
    "outputId": "a532758b-30f0-4b0f-aed4-b65f922e5042"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenizing train dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train:   0%|          | 0/903 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53d8d239999e469cb6850103e801c864"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\u2713 Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")"
   ],
   "metadata": {
    "id": "BXof1zja88O6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "referenced_widgets": [
      "7e8ebf414bc84468aa8dfe5cd769c109",
      "d6ada58d30384f828ee6f2b114aa9fdf",
      "85fe0a43d986433687ca5fc28b3a945e",
      "dac79eedb7074358bb856b12b01b0729",
      "d74fd5cd20144bb7b5c56170d67ae79d",
      "9e7684a87a484b468a6d3095ddc4293a",
      "2773706e56574a77a3b173eb3fd3710d",
      "489be62562dc4e9bb557f0061c977cf1",
      "f78e71fb215543e1b1f76d41d6b88318",
      "70447924f7474642a6ee5e64f750004c",
      "ec8f94304244458bbf73b90d678c6e30"
     ]
    },
    "outputId": "4d5550c5-f7eb-45e8-83a7-7ee8f34496fa"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenizing validation dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8ebf414bc84468aa8dfe5cd769c109"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Show sample stats (pre-training)\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\n\u2713 Sample stats:\")\n",
    "print(f\"  - Input length: {len(sample['input_ids'])} tokens\")\n",
    "print(f\"  - Attention tokens: {sum(sample['attention_mask'])} tokens\")\n",
    "print(f\"  - Truncated: {'Yes' if len(sample['input_ids']) == 2 ** 15 else 'No'}\")\n"
   ],
   "metadata": {
    "id": "dYYvowak9DRx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32a04f82-c70b-466d-ff71-71327fb9e711"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2713 Sample stats:\n",
      "  - Input length: 1382 tokens\n",
      "  - Attention tokens: 1382 tokens\n",
      "  - Truncated: No\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n[6/8] Configuring training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final-summary\",\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # Memory-optimized batch settings\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Learning settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Efficiency\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ],
   "metadata": {
    "id": "SPZgzWLy9Eww",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c967dad4-8f27-4e6d-c71b-bca0dfcd2bef"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[6/8] Configuring training...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\u2713 Training configuration:\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Total training steps: ~{len(train_dataset) * 3 // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ],
   "metadata": {
    "id": "CzKAZpYU9GX8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3038088e-7689-4926-83c2-903c58521c0e"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Training configuration:\n",
      "  - Effective batch size: 16\n",
      "  - Total training steps: ~169\n",
      "  - Learning rate: 0.0002\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Data collator with dynamic padding\n",
    "print(\"\\n[7/8] Creating data collator...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Efficient padding for GPU\n",
    ")"
   ],
   "metadata": {
    "id": "DG_BRUVo9II8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46a260dc-d3bc-4cc8-e2b7-d04ebbe0d109"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[7/8] Creating data collator...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ],
   "metadata": {
    "id": "z8tofMzP9JcZ"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "print(\"\\n[8/8] Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING IN PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "id": "lktCWD8F9K0-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "outputId": "72fe1a62-8948-4876-a061-2cc1ec5e5edd"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[8/8] Starting training...\n",
      "============================================================\n",
      "TRAINING IN PROGRESS\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 1:51:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.998300</td>\n",
       "      <td>2.143472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=2.1060634607460065, metrics={'train_runtime': 6778.3477, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.025, 'total_flos': 2.0566782399297946e+17, 'train_loss': 2.1060634607460065, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(\"./final-summary\")\n",
    "tokenizer.save_pretrained(\"./final-summary\")"
   ],
   "metadata": {
    "id": "gT_CZX7E9MTa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c67636ed-9ea4-40fb-eeb6-178abf1a543f"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./final-summary/tokenizer_config.json',\n",
       " './final-summary/special_tokens_map.json',\n",
       " './final-summary/chat_template.jinja',\n",
       " './final-summary/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!zip -r ./final-summary.zip ./final-summary"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgb769MhBbWc",
    "outputId": "9e3b363f-da44-4080-f7aa-6cabf8e4665d"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: final-summary/ (stored 0%)\n",
      "  adding: final-summary/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-100/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-100/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-100/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-100/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-100/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-100/trainer_state.json (deflated 70%)\n",
      "  adding: final-summary/checkpoint-100/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-100/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-100/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-100/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/checkpoint-100/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/ (stored 0%)\n",
      "  adding: final-summary/checkpoint-171/README.md (deflated 65%)\n",
      "  adding: final-summary/checkpoint-171/adapter_config.json (deflated 58%)\n",
      "  adding: final-summary/checkpoint-171/training_args.bin (deflated 53%)\n",
      "  adding: final-summary/checkpoint-171/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/checkpoint-171/rng_state.pth (deflated 26%)\n",
      "  adding: final-summary/checkpoint-171/trainer_state.json (deflated 73%)\n",
      "  adding: final-summary/checkpoint-171/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/checkpoint-171/optimizer.pt (deflated 11%)\n",
      "  adding: final-summary/checkpoint-171/scheduler.pt (deflated 62%)\n",
      "  adding: final-summary/checkpoint-171/adapter_model.safetensors (deflated 8%)\n",
      "  adding: final-summary/checkpoint-171/tokenizer.json (deflated 85%)\n",
      "  adding: final-summary/special_tokens_map.json (deflated 63%)\n",
      "  adding: final-summary/tokenizer_config.json (deflated 96%)\n",
      "  adding: final-summary/chat_template.jinja (deflated 71%)\n",
      "  adding: final-summary/adapter_model.safetensors (deflated 7%)\n",
      "  adding: final-summary/tokenizer.json (deflated 85%)\n"
     ]
    }
   ]
  }
 ]
}