{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install pandas datasets"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install transformers torch",
   "id": "ada6ff246eb7586d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install xformers trl peft accelerate bitsandbytes",
   "id": "2372ae28c16fc3b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch"
   ],
   "id": "adad89df7ec153b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"We would be using this device:\", device)"
   ],
   "id": "f8f310dd6309d904"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Dataset",
   "id": "296c236a9d6a2e47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load JSONL data (Custom Dataset)\n",
    "print(\"\\n2-[1/8] Loading dataset...\")\n",
    "data = []\n",
    "with open(\"custom_dataset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "custom_dataset = Dataset.from_dict({\n",
    "    \"messages\": [item[\"messages\"] for item in data]\n",
    "})\n",
    "\n",
    "print(f\"✓ Loaded {len(custom_dataset)} examples\")"
   ],
   "id": "42b8ae4fe4f70311"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing the Train and Test Dataset\n",
    "\n",
    "we have decided to split 90% for training and 10% for testing"
   ],
   "id": "e24b38aa050ff81e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split dataset\n",
    "split_dataset = custom_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "print(f\"✓ Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
   ],
   "id": "d8d70cada36e9678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n[2/8] Configuring 8-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ],
   "id": "a310c7619d5fb446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## Note\n",
    "\n",
    "we have login inside hugging face and make sure we request for access to use the LLaMA 3.2 3B model"
   ],
   "id": "ba6b21343a150d3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!huggingface-cli login",
   "id": "300b2c43cb2ae8c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load model and tokenizer\n",
    "print(\"\\n[3/8] Loading LLaMA 3.2 3B model...\")\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# LLaMA models usually do not have a pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded in 8-bit\")\n",
    "print(\"✓ Model size: ~3B parameters\")"
   ],
   "id": "816b6dbb67a3f406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare for k-bit training\n",
    "print(\"\\n[4/8] Preparing model for QLoRA...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "# (Optional but recommended) disable cache during training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "id": "b309b94758c63a53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "print(\"✓ LoRA adapters added\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenization function with proper chat formatting\n",
    "print(\"\\n[5/8] Preparing tokenization...\")"
   ],
   "id": "23bf65410c3353da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_and_tokenize(example, force=4096):\n",
    "    # Apply LLaMA chat template\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize (NO padding, NO manual labels)\n",
    "    tokenized = tokenizer(\n",
    "        formatted_text,\n",
    "        truncation=True,\n",
    "        max_length=force,   # IMPORTANT: increase for long transcripts\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    return tokenized\n"
   ],
   "id": "6b118b8c71b8ac2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Note\n",
    "\n",
    "we need to set the max_length for the tokenization function to 32768 for the custom dataset"
   ],
   "id": "8417378e9c246156"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.071580300Z",
     "start_time": "2025-12-16T18:03:09.043362700Z"
    }
   },
   "cell_type": "code",
   "source": "format_and_tokenize_for_cds = lambda x: format_and_tokenize(x, force=2 ** 15)",
   "id": "ade5964302e23d6f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T18:03:09.098147400Z",
     "start_time": "2025-12-16T18:03:09.076244700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply tokenization\n",
    "print(\"✓ Tokenizing train dataset...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n"
   ],
   "id": "9c8323333f3edffa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"✓ Tokenizing validation dataset...\")\n",
    "val_dataset = val_dataset.map(\n",
    "    format_and_tokenize_for_cds,\n",
    "    remove_columns=[\"messages\"],\n",
    "    batched=False,\n",
    "    desc=\"Tokenizing validation\"\n",
    ")"
   ],
   "id": "58b20211ea99f57b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Show sample stats (pre-training)\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"\\n✓ Sample stats:\")\n",
    "print(f\"  - Input length: {len(sample['input_ids'])} tokens\")\n",
    "print(f\"  - Attention tokens: {sum(sample['attention_mask'])} tokens\")\n",
    "print(f\"  - Truncated: {'Yes' if len(sample['input_ids']) == 2 ** 15 else 'No'}\")\n"
   ],
   "id": "874017a44748d9b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n[6/8] Configuring training...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3.2-3b-qlora-summary\",\n",
    "    num_train_epochs=6,\n",
    "\n",
    "    # Memory-optimized batch settings\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # Learning settings\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # Optimization\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Efficiency\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")"
   ],
   "id": "7b8722b1358c4743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"✓ Training configuration:\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Total training steps: ~{len(train_dataset) * 3 // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")"
   ],
   "id": "6f810b8f30717975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data collator with dynamic padding\n",
    "print(\"\\n[7/8] Creating data collator...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8  # Efficient padding for GPU\n",
    ")"
   ],
   "id": "e348a54ee3b86e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ],
   "id": "a0771aa53dfc6c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Start training\n",
    "print(\"\\n[8/8] Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING IN PROGRESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "d85a329ba6398351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save final model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaving model...\")\n",
    "trainer.save_model(\"./llama3.2-3b-qlora-summary\")\n",
    "tokenizer.save_pretrained(\"./llama3.2-3b-qlora-summary\")"
   ],
   "id": "9c558ddb67925379"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
