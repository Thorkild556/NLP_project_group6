{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfabd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thorkildkappel/Desktop/7. sem/NLP /project/NLP_project_group6/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924c10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_dataset(\"trl-lib/tldr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa557d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}\n",
    "for set_type in data_set.keys():\n",
    "  ds[set_type] = pd.DataFrame(data_set[set_type])\n",
    "  ds[set_type][\"prompt_post\"] = ds[set_type].prompt.str.extract(\n",
    "      r'POST: ((.|\\n)*)\\nTL;DR:', expand=False\n",
    "  ).iloc[:, 0]\n",
    "  ds[set_type][\"prompt_title\"] = ds[set_type].prompt.str.extract(\n",
    "    r'TITLE: ((.|\\n)*)\\n\\nPOST:', expand=False\n",
    "  ).iloc[:, 0]\n",
    "  ds[set_type][\"subreddit\"] = ds[set_type].prompt.str.extract(\n",
    "    r'SUBREDDIT: ((.|\\n)*)\\n\\nTITLE:', expand=False\n",
    "  ).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd488919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'completion', 'prompt_post', 'prompt_title', 'subreddit'], dtype='object')\n",
      "My 25 year old son has a job which means he does 12 hour nightshifts 3-4 times a week, including weekends. After his nightshifts, he comes to my house an sleeps in the spare bedroom. He sleeps in my house because he says at his own house, it's too noisy. \n",
      "\n",
      "In his own house, he has his partner, who is mostly home during weekdays as she only works weekends. They then have a one year old, then his partner has a 14 year old girl who - if it's the weekends or after school, always has at least 2 friends with her. It's the group of girls who are the noisiest. So the house is very noisy. \n",
      "\n",
      "I don't have a problem with my son sleeping in my spare room, but I think as his mother I should try and encourage him to have a better arrangement set up in his own home so he can sleep there, for his sake. For example, if the 14 year old does have friends over, she could only have them downstairs, and they'd not be allowed upstairs when my son is sleeping. My son's partner says since the 14 year old's bedroom is upstairs she doesnt think she could tell her daughter and her friends they couldnt come upstairs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "train_set = ds[\"train\"]\n",
    "train_set[\"prompt_post\"] = train_set[\"prompt_post\"].str.replace(r'\\s*TL;DR:\\s*', '', regex=True)\n",
    "train_set[\"completion\"] = [\n",
    "    re.sub(r'\\n[ \\t]+', '\\n', re.sub(r'^[ \\t]+', '', t))\n",
    "    for t in train_set[\"completion\"]\n",
    "]\n",
    "print(train_set.columns)\n",
    "print(train_set.iloc[1234][\"prompt_post\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0675a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/ksmd9z810hv7hd63k2mmtgzr0000gn/T/ipykernel_8676/1256756349.py:8: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  downsampled_other = other_df.groupby('subreddit', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Separate AskReddit from other subreddits\n",
    "askreddit_df = train_set[train_set['subreddit'] == 'r/AskReddit']\n",
    "other_df = train_set[train_set['subreddit'] != 'r/AskReddit']\n",
    "\n",
    "# Downsample other subreddits to max 2000 each\n",
    "downsampled_other = other_df.groupby('subreddit', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), 2000), random_state=42)\n",
    ")\n",
    "\n",
    "# Combine back together\n",
    "train_set = pd.concat([askreddit_df, downsampled_other], ignore_index=True)\n",
    "train_set = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# print(train_set['subreddit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a268985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41773\n",
      "41601\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "train_set = train_set.drop_duplicates([\"prompt_post\"])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac23372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a981f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41601, 62384)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfidf_mat = tfidf.fit_transform(train_set[\"prompt_post\"])\n",
    "print(tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479edb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any size-1 clusters? False\n",
      "Clustered: 20487 / 41601\n",
      "Unassigned: 21114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def knn_greedy_clustering(tfidf_matrix, min_size=2, max_size=4, \n",
    "                         threshold=0.7, n_neighbors=20):\n",
    "    \"\"\"\n",
    "    Use k-NN for fast neighbor finding, then greedy cluster formation\n",
    "    \"\"\"\n",
    "    n = tfidf_matrix.shape[0]\n",
    "    \n",
    "    # Fit k-NN model (cosine similarity)\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(n_neighbors + 1, n),  # +1 for self\n",
    "                           metric='cosine', \n",
    "                           algorithm='auto')\n",
    "    nbrs.fit(tfidf_matrix)\n",
    "    \n",
    "    # Get neighbors for all samples at once\n",
    "    distances, indices = nbrs.kneighbors(tfidf_matrix)\n",
    "    similarities = 1 - distances  # Convert distance to similarity\n",
    "    \n",
    "    # Greedy clustering using precomputed neighbors\n",
    "    assigned = np.zeros(n, dtype=bool)\n",
    "    clusters = []\n",
    "    \n",
    "    # Process in order of maximum similarity (samples with strong matches first)\n",
    "    max_sims = similarities[:, 1].copy()  # Skip self (index 0)\n",
    "    order = np.argsort(max_sims)[::-1]\n",
    "    \n",
    "    for idx in order:\n",
    "        if assigned[idx]:\n",
    "            continue\n",
    "        \n",
    "        cluster = [int(idx)]  # Convert to Python int immediately\n",
    "        \n",
    "        # Add neighbors that are unassigned and above threshold\n",
    "        # Start from index 1 to skip self (index 0 is always the sample itself)\n",
    "        for neighbor_idx, sim in zip(indices[idx][1:], similarities[idx][1:]):\n",
    "            neighbor_idx = int(neighbor_idx)  # Convert to Python int\n",
    "            \n",
    "            if neighbor_idx != idx and not assigned[neighbor_idx] and sim >= threshold and sim <0.95:\n",
    "                cluster.append(neighbor_idx)\n",
    "                if len(cluster) >= max_size:\n",
    "                    break\n",
    "        \n",
    "        # Only add cluster if it meets minimum size\n",
    "        if len(cluster) >= min_size:\n",
    "            clusters.append(cluster)\n",
    "            assigned[cluster] = True\n",
    "        # If cluster too small, leave sample unassigned for now\n",
    "    \n",
    "    return clusters, assigned\n",
    "\n",
    "# Usage\n",
    "clusters, assigned = knn_greedy_clustering(\n",
    "    tfidf_mat, \n",
    "    min_size=2, \n",
    "    max_size=4, \n",
    "    threshold=0.3,\n",
    "    n_neighbors=50\n",
    ")\n",
    "\n",
    "# Verify no single-element clusters\n",
    "print(f\"Any size-1 clusters? {any(len(c) == 1 for c in clusters)}\")\n",
    "print(f\"Clustered: {assigned.sum()} / {len(assigned)}\")\n",
    "print(f\"Unassigned: {(~assigned).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aaf1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster size distribution:\n",
      "  Size 2: 3121 clusters\n",
      "  Size 3: 1047 clusters\n",
      "  Size 4: 2776 clusters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cluster_sizes = [len(c) for c in clusters]\n",
    "unique, counts = np.unique(cluster_sizes, return_counts=True)\n",
    "\n",
    "print(\"Cluster size distribution:\")\n",
    "for size, count in zip(unique, counts):\n",
    "    print(f\"  Size {size}: {count} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c89889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38596, 21353, 29682, 16046]\n"
     ]
    }
   ],
   "source": [
    "print(clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad53c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit a careless jaywalker at around 20mph going thru an intersection on my green. I drove abiding all traffic laws. What can be the outcome?\n",
      "2 jobs, the first $65k train driver or $?? IT position with no formal job title yet but he did mention working under him as an admin assistant.\n",
      "I need a new laptop and I might be fed up with Windows. Show me what's best all around without any specific criteria.\n"
     ]
    }
   ],
   "source": [
    "print(train_set['completion'].iloc[36376])\n",
    "#print(train_set['completion'].iloc[34728])\n",
    "print(train_set['completion'].iloc[18913])\n",
    "print(train_set['completion'].iloc[6146])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d78af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, there really is a subreddit for everything. Ok well if someone who knows about this could help me out i would appreciate it.\n",
      "\n",
      "I few month ago I heard about how Ireland adopted A.C.T.A ( or was it sopa?) anyway just wile skimming and article I heard that this gave them the right to search mp3 players, ipods, and laptops for pirated music in the same way they can search your bag for drugs. now i didn't pay attention to the credibility of the source at the time because i hadn't seen going to Ireland in the foreseeable future but something has come up and I am headed there this December. If this is true it will mean the difference between be bringing my ipod and laptop or not.\n",
      "\n",
      "I have been doing some research but it is the internet and its difficult to find a credible answer to any obscure question like this.\n",
      "\n",
      "So this guy was my friend in college. I am really good friends with his girlfriend and to make a long story short we betrayed him and we had sex. So she feels really bad about this and regrets it so she tells him that she cheated on him but wants to work things out and he doesn't break up with her.\n",
      "\n",
      "2 months later she finally tells him we had sex and so he tells me he wants to talk. I get on skype and he tells me that he is at my local high school and wants to meet up (he drove 3 hours and skipped work). I tell him that I don't think it was a good idea. I was playing basketball at the time with a bunch of friends and I told him he could come to where I am.\n",
      "\n",
      "He does and goes into the church which is near the basketball court and I go in and he is sitting on a pew opposite and we talk. He keeps asking me if there is anything he should know I told him no. He tells me wrong answer and I ask him what is the right answer. He then beats around the bush talking about if there is anything i want confess yadayada. So I tell him that you already know everything and finally he just tells me he wants to hear me say it.\n",
      "\n",
      "So I told him we had sex, he then stands up and walks over to me and tries to punch me in the face but I block it. He then starts trying to get me on the ground and starts fighting me in the church. I got away and then walked out of the church towards my friends and he gets in his car and leaves.\n",
      "\n",
      "Anyways what the fuck.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_set[\"prompt_post\"].iloc[11304])\n",
    "print(train_set[\"prompt_post\"].iloc[7008])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00263f",
   "metadata": {},
   "source": [
    "## Create the new dataset based on the clusters and the prepends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3fa5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Go one directory up from notebook location\n",
    "notebook_dir = Path().resolve()\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "# Add to path\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.tldr_prepend import Prepends\n",
    "\n",
    "processed = {\"transscript\": [], \"TLDR\": []}\n",
    "\n",
    "prep = Prepends()\n",
    "\n",
    "for group in clusters:\n",
    "    processed[\"TLDR\"].append(\"\")\n",
    "    processed[\"transscript\"].append(\"\")\n",
    "    for i, text_idx in enumerate(group):\n",
    "        processed[\"TLDR\"][-1] += f\"{prep.get_random_prepend(i+1)}\\n{train_set['completion'].iloc[text_idx]}\\n\\n\"\n",
    "        # format transscripts\n",
    "        processed[\"transscript\"][-1] += f\"TITLE_OF_VIDEO_{i+1}: {train_set[\"prompt_title\"].iloc[text_idx]}\\n\"\n",
    "        processed[\"transscript\"][-1] += f\"TRANSCRIPT_OF_VIDEO_RESULT_{i + 1}: {train_set['prompt_post'].iloc[text_idx]}\\n\"\n",
    "\n",
    "processed_df = pd.DataFrame(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32bfd77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6944"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dd9d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SYSTEM_PROMPT = \"You summarize multiple video transcripts into concise, factual summaries.\"\n",
    "\n",
    "with open(\"../data/proc_tldr.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in processed_df.iterrows():\n",
    "        example = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": row[\"transscript\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"TLDR\"]},\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(example, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7af87a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines to remove from microsoft filter\n",
    "lines_to_remove = [15, 20, 24, 36, 45, 52, 54, 63, 83, 110, 131, 133, 143, 160, 164, 168, 190, 191, 224, 225, 228, 229, 233, 238, 239, 247, 261, 268, 291, 306, 307, 308, 309, 314, 321, 323, 333, 338, 345, 346, 348, 393, 395, 396, 404, 406, 412, 413, 416, 417, 420, 421, 424, 429, 430, 431, 432, 440, 441, 442, 445, 446, 449, 450, 451, 452, 456, 457, 458, 470, 471, 477, 480, 490, 501, 502, 510, 512, 520, 533, 542, 551, 564, 577, 587, 594, 602, 603, 609, 619, 620, 637, 649, 660, 668, 691, 705, 712, 726, 730, 40, 136, 197, 287, 303, 320, 362, 386, 422, 516, 526, 559, 631, 638, 655, 665, 681, 721, 770, 780, 806, 807, 814, 818, 856, 867, 889, 916, 944, 955, 994, 1000, 1041, 1055, 1283, 1384, 1392, 1406, 1417, 1464, 1693, 1760, 1789, 1790, 1814, 1835, 1853, 1882, 2020, 2024, 2044, 2072, 2097, 2160, 2284, 2322, 2389, 2405, 2653, 2670, 2693, 2804, 2857, 2931, 3017, 3111, 3118, 3168, 3273, 3283, 3467, 3486, 3565, 3640, 3856, 3957, 3972, 3976, 3987, 4101, 4111, 4134, 4146, 4184, 4207, 4298, 4408, 4426, 4469, 4541, 4583, 4662, 4846, 4861, 5042, 5079, 5146, 5159, 5197, 5203, 50, 187, 240, 278, 296, 318, 359, 360, 380, 381, 408, 409, 411, 548, 549, 588, 610, 694, 701, 716, 735, 743, 778, 784, 825, 826, 901, 914, 1016, 1022, 1044, 1054, 1082, 1115, 1124, 1129, 1135, 1210, 1228, 1309, 1337, 1347, 1359, 1362, 1368, 1393, 1395, 1412, 1436, 1441, 1473, 1499, 1512, 1523, 1552, 1554, 1571, 1646, 1648, 1650, 1663, 1711, 1765, 1777, 1784, 1818, 1834, 1944, 1996, 2093, 2098, 2133, 2171, 2199, 2251, 2261, 2367, 2395, 2413, 2419, 2466, 2475, 2489, 2527, 2618, 2646, 2673, 2674, 2702, 2710, 2713, 2721, 2724, 2729, 2751, 2769, 2826, 2953, 2994, 3018, 124, 203, 678, 680, 720, 968, 1100, 1163, 1269, 1293, 1632, 1636, 1672, 1718, 1729, 1806, 1923, 2057, 2069, 2308, 2408, 2519, 2594, 3050, 3233, 3420, 3434, 3468, 3522, 3524, 3678, 3684, 3753, 3790, 4043, 4213, 4261, 4297, 4334, 4375, 4490, 4522, 4523, 4673, 4753, 5109, 5132, 5170, 5508, 5626, 5635, 5910, 5911, 5974, 6343, 6463, 6516, 6737, 6905]\n",
    "for i in range(len(lines_to_remove)):\n",
    "    lines_to_remove[i] -=1\n",
    "\n",
    "processed_df = processed_df.drop(lines_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87012e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transscript    TITLE_OF_VIDEO_1: I have a job offer dilemma, ...\n",
      "TLDR           Video 1 covered:\\nHave one job offer on the ta...\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_df.iloc[14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0871808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f08a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SYSTEM_PROMPT = \"You summarize multiple video transcripts into concise, factual summaries.\"\n",
    "\n",
    "with open(\"../data/proc_tldr_filtered.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in processed_df.iterrows():\n",
    "        example = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": row[\"transscript\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"TLDR\"]},\n",
    "            ]\n",
    "        }\n",
    "        f.write(json.dumps(example, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
